"use strict";(globalThis.webpackChunkcoo_llm_docs=globalThis.webpackChunkcoo_llm_docs||[]).push([[849],{6164:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"\ud83d\ude80 User Guide","collapsed":false,"items":[{"type":"link","href":"/docs/Intro/Overview","label":"COO-LLM Overview","docId":"Intro/Overview","unlisted":false},{"type":"link","href":"/docs/Guides/Configuration","label":"Configuration","docId":"Guides/Configuration","unlisted":false},{"type":"link","href":"/docs/Guides/Deployment","label":"Deployment","docId":"Guides/Deployment","unlisted":false},{"type":"link","href":"/docs/Guides/Providers","label":"Providers","docId":"Guides/Providers","unlisted":false}],"collapsible":true},{"type":"category","label":"\ud83d\udd27 Developer Guide","collapsed":false,"items":[{"type":"link","href":"/docs/Intro/Architecture","label":"Architecture","docId":"Intro/Architecture","unlisted":false},{"type":"link","href":"/docs/Reference/API","label":"API Reference","docId":"Reference/API","unlisted":false},{"type":"link","href":"/docs/Reference/Balancer","label":"Load Balancer","docId":"Reference/Balancer","unlisted":false},{"type":"link","href":"/docs/Reference/Storage","label":"Storage","docId":"Reference/Storage","unlisted":false},{"type":"link","href":"/docs/Reference/Logging","label":"Logging","docId":"Reference/Logging","unlisted":false},{"type":"link","href":"/docs/Contributing/Guidelines","label":"Contributing","docId":"Contributing/Guidelines","unlisted":false},{"type":"link","href":"/docs/Contributing/Changelog","label":"Changelog","docId":"Contributing/Changelog","unlisted":false}],"collapsible":true}]},"docs":{"Contributing":{"id":"Contributing","title":"Contributing","description":"We welcome contributions to COO-LLM! This section contains information for contributors."},"Contributing/Changelog":{"id":"Contributing/Changelog","title":"Changelog","description":"All notable changes to COO-LLM will be documented in this file.","sidebar":"tutorialSidebar"},"Contributing/Guidelines":{"id":"Contributing/Guidelines","title":"Contributing","description":"Thank you for your interest in contributing to COO-LLM! This document provides guidelines and information for contributors.","sidebar":"tutorialSidebar"},"Guides":{"id":"Guides","title":"Guides","description":"This section contains practical guides for using and deploying COO-LLM."},"Guides/Configuration":{"id":"Guides/Configuration","title":"Configuration","description":"COO-LLM uses YAML configuration files for all settings. The configuration is hierarchical and supports environment variable substitution, validation, and hot-reload.","sidebar":"tutorialSidebar"},"Guides/Deployment":{"id":"Guides/Deployment","title":"Deployment","description":"This guide covers deploying COO-LLM in various environments, from development to production.","sidebar":"tutorialSidebar"},"Guides/Providers":{"id":"Guides/Providers","title":"Providers","description":"COO-LLM supports multiple LLM providers through a plugin-based architecture. Each provider implements a common interface for seamless integration.","sidebar":"tutorialSidebar"},"Intro":{"id":"Intro","title":"Introduction","description":"Welcome to COO-LLM! This section provides an overview of what COO-LLM is and how it works."},"Intro/Architecture":{"id":"Intro/Architecture","title":"Architecture","description":"This document describes the high-level architecture of COO-LLM, including component interactions and design decisions.","sidebar":"tutorialSidebar"},"Intro/Overview":{"id":"Intro/Overview","title":"COO-LLM Overview","description":"COO-LLM is an intelligent reverse proxy and load balancer for Large Language Model (LLM) APIs. It provides a unified, OpenAI-compatible interface to multiple LLM providers while intelligently distributing requests across API keys and providers based on performance, cost, and rate limits.","sidebar":"tutorialSidebar"},"README":{"id":"README","title":"COO-LLM Documentation","description":"This directory contains the documentation for COO-LLM, built with Docusaurus."},"Reference":{"id":"Reference","title":"Reference","description":"This section contains detailed technical reference documentation."},"Reference/API":{"id":"Reference/API","title":"API Reference","description":"COO-LLM provides OpenAI-compatible REST APIs for LLM interactions, plus administrative endpoints for management.","sidebar":"tutorialSidebar"},"Reference/Balancer":{"id":"Reference/Balancer","title":"Load Balancer","description":"The load balancer is the core intelligence of COO-LLM, responsible for selecting optimal provider and API key combinations based on performance, cost, and availability.","sidebar":"tutorialSidebar"},"Reference/Logging":{"id":"Reference/Logging","title":"Logging","description":"COO-LLM provides structured logging with file output and Prometheus metrics integration.","sidebar":"tutorialSidebar"},"Reference/Storage":{"id":"Reference/Storage","title":"Storage","description":"COO-LLM uses pluggable storage backends for runtime metrics and caching. The system supports Redis, in-memory, HTTP API, and file-based storage.","sidebar":"tutorialSidebar"}}}}')}}]);