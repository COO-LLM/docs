"use strict";(globalThis.webpackChunkcoo_llm_docs=globalThis.webpackChunkcoo_llm_docs||[]).push([[2771],{6057:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"Guides/Providers","title":"Providers","description":"COO-LLM supports multiple LLM providers through a plugin-based architecture. Each provider implements a common interface for seamless integration.","source":"@site/content/Guides/Providers.md","sourceDirName":"Guides","slug":"/Guides/Providers","permalink":"/docs/Guides/Providers","draft":false,"unlisted":false,"editUrl":"https://github.com/coo-llm/coo-llm-main/tree/main/docs/content/content/Guides/Providers.md","tags":[{"inline":true,"label":"user-guide","permalink":"/docs/tags/user-guide"},{"inline":true,"label":"providers","permalink":"/docs/tags/providers"}],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"tags":["user-guide","providers"]},"sidebar":"tutorialSidebar","previous":{"title":"Deployment","permalink":"/docs/Guides/Deployment"},"next":{"title":"Architecture","permalink":"/docs/Intro/Architecture"}}');var s=i(4848),t=i(8453);const o={sidebar_position:4,tags:["user-guide","providers"]},l="Providers",d={},c=[{value:"Provider Architecture",id:"provider-architecture",level:2},{value:"Supported Providers",id:"supported-providers",level:2},{value:"OpenAI",id:"openai",level:3},{value:"Google Gemini",id:"google-gemini",level:3},{value:"Anthropic Claude",id:"anthropic-claude",level:3},{value:"Using Custom Endpoints",id:"using-custom-endpoints",level:2},{value:"Provider Interface",id:"provider-interface",level:2},{value:"Request Structure",id:"request-structure",level:3},{value:"Response Structure",id:"response-structure",level:3},{value:"Configuration Structure",id:"configuration-structure",level:3},{value:"Adding Custom Providers",id:"adding-custom-providers",level:2},{value:"Provider-Specific Features",id:"provider-specific-features",level:2},{value:"OpenAI Features",id:"openai-features",level:3},{value:"Gemini Features",id:"gemini-features",level:3},{value:"Claude Features",id:"claude-features",level:3},{value:"Pricing Integration",id:"pricing-integration",level:2},{value:"Rate Limiting",id:"rate-limiting",level:2},{value:"Error Handling",id:"error-handling",level:2},{value:"Monitoring",id:"monitoring",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Key Management",id:"key-management",level:3},{value:"Cost Optimization",id:"cost-optimization",level:3},{value:"Reliability",id:"reliability",level:3},{value:"Security",id:"security",level:3}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"providers",children:"Providers"})}),"\n",(0,s.jsx)(n.p,{children:"COO-LLM supports multiple LLM providers through a plugin-based architecture. Each provider implements a common interface for seamless integration."}),"\n",(0,s.jsx)(n.h2,{id:"provider-architecture",children:"Provider Architecture"}),"\n",(0,s.jsx)(n.mermaid,{value:"flowchart TD\n    classDef interface fill:#ffc107,color:#000,stroke:#000,stroke-width:2px\n    classDef provider fill:#dc3545,color:#fff,stroke:#fff,stroke-width:2px\n    classDef registry fill:#ffc107,color:#000,stroke:#000,stroke-width:2px\n    classDef external fill:#007bff,color:#fff,stroke:#fff,stroke-width:2px\n\n    A[LLMProvider Interface<br/>interface.go]:::interface\n    A --\x3e C[ListModels<ctx><br/> Array<string>]:::interface\n    A --\x3e B[Generate <ctx, req> <br/>LLMRequest \u2192 LLMResponse]:::interface\n\n    D[Provider Registry<br/>registry.go]:::registry\n    D --\x3e E[Load from Config<br/>llm_providers]:::registry\n    D --\x3e F[Provider Instances<br/>OpenAI, Gemini, Claude]:::registry\n\n    G[OpenAI Provider<br/>openai.go]:::provider\n    G --\x3e H[OpenAI API<br/>chat/completions]:::external\n\n    I[Gemini Provider<br/>gemini.go]:::provider\n    I --\x3e J[Gemini API<br/>generateContent]:::external\n\n    K[Claude Provider<br/>claude.go]:::provider\n    K --\x3e L[Claude API<br/>messages]:::external\n\n    B --\x3e G\n    B --\x3e I\n    B --\x3e K\n\n    E --\x3e G\n    E --\x3e I\n    E --\x3e K"}),"\n",(0,s.jsx)(n.h2,{id:"supported-providers",children:"Supported Providers"}),"\n",(0,s.jsx)(n.h3,{id:"openai",children:"OpenAI"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Provider Type:"})," ",(0,s.jsx)(n.code,{children:"openai"})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Configuration:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'llm_providers:\n  - id: "openai-prod"\n    type: "openai"\n    api_keys: ["${OPENAI_KEY_1}", "${OPENAI_KEY_2}"]\n    base_url: "https://api.openai.com"\n    model: "gpt-4o"\n    pricing:\n      input_token_cost: 0.002\n      output_token_cost: 0.01\n    limits:\n      req_per_min: 200\n      tokens_per_min: 100000\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Supported Models:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"gpt-4"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"gpt-4-turbo"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"gpt-4o"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"gpt-3.5-turbo"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"gpt-3.5-turbo-instruct"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Features:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Chat completions with conversation history"}),"\n",(0,s.jsx)(n.li,{children:"Token usage tracking"}),"\n",(0,s.jsx)(n.li,{children:"Error handling and retries"}),"\n",(0,s.jsx)(n.li,{children:"Rate limit management"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Rate Limits:"})," Based on OpenAI tier (see ",(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/rate-limits",children:"OpenAI docs"}),")"]}),"\n",(0,s.jsx)(n.h3,{id:"google-gemini",children:"Google Gemini"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Provider Type:"})," ",(0,s.jsx)(n.code,{children:"gemini"})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Configuration:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'llm_providers:\n  - id: "gemini-prod"\n    type: "gemini"\n    api_keys: ["${GEMINI_KEY_1}"]\n    base_url: "https://generativelanguage.googleapis.com"\n    model: "gemini-1.5-pro"\n    pricing:\n      input_token_cost: 0.00025\n      output_token_cost: 0.0005\n    limits:\n      req_per_min: 150\n      tokens_per_min: 80000\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Supported Models:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"gemini-1.5-pro"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"gemini-1.5-flash"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"gemini-1.0-pro"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"gemini-2.0-flash-exp"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"gemini-2.0-pro-exp"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Features:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Chat completions"}),"\n",(0,s.jsx)(n.li,{children:"Token usage tracking"}),"\n",(0,s.jsx)(n.li,{children:"Error handling"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Rate Limits:"})," Varies by tier (see ",(0,s.jsx)(n.a,{href:"https://ai.google.dev/docs/rate-limits",children:"Gemini docs"}),")"]}),"\n",(0,s.jsx)(n.h3,{id:"anthropic-claude",children:"Anthropic Claude"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Provider Type:"})," ",(0,s.jsx)(n.code,{children:"claude"})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Configuration:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'llm_providers:\n  - id: "claude-prod"\n    type: "claude"\n    api_keys: ["${CLAUDE_KEY_1}"]\n    base_url: "https://api.anthropic.com"\n    model: "claude-3-opus"\n    pricing:\n      input_token_cost: 0.015\n      output_token_cost: 0.075\n    limits:\n      req_per_min: 100\n      tokens_per_min: 60000\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Supported Models:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"claude-3-opus"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"claude-3-sonnet"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"claude-3-haiku"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"claude-2.1"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Features:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Chat completions"}),"\n",(0,s.jsx)(n.li,{children:"Token usage tracking"}),"\n",(0,s.jsx)(n.li,{children:"Error handling"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Rate Limits:"})," Varies by model (see ",(0,s.jsx)(n.a,{href:"https://docs.anthropic.com/claude/docs/rate-limits",children:"Anthropic docs"}),")"]}),"\n",(0,s.jsx)(n.h2,{id:"using-custom-endpoints",children:"Using Custom Endpoints"}),"\n",(0,s.jsxs)(n.p,{children:["For models that follow OpenAI, Gemini, or Claude API standards, you can use the respective provider type and specify a custom ",(0,s.jsx)(n.code,{children:"base_url"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'llm_providers:\n  - id: "custom-openai-compatible"\n    type: "openai"  # Use openai type for OpenAI-compatible APIs\n    api_keys: ["${CUSTOM_KEY}"]\n    base_url: "https://your-custom-endpoint.com/v1"  # Custom URL\n    model: "your-model"\n    pricing:\n      input_token_cost: 0.001\n      output_token_cost: 0.002\n    limits:\n      req_per_min: 100\n      tokens_per_min: 50000\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Supported Types for Custom Endpoints:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"openai"}),": For OpenAI-compatible APIs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"gemini"}),": For Gemini-compatible APIs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"claude"}),": For Claude-compatible APIs"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"If your model doesn't follow these standards, consider using the official SDKs directly or contributing a new provider implementation."}),"\n",(0,s.jsx)(n.h2,{id:"provider-interface",children:"Provider Interface"}),"\n",(0,s.jsxs)(n.p,{children:["All providers implement the ",(0,s.jsx)(n.code,{children:"LLMProvider"})," interface defined in ",(0,s.jsx)(n.code,{children:"internal/provider/interface.go"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-go",children:"type LLMProvider interface {\n    Name() string\n    Generate(ctx context.Context, req *LLMRequest) (*LLMResponse, error)\n    ListModels(ctx context.Context) ([]string, error)\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"request-structure",children:"Request Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-go",children:'type LLMRequest struct {\n    Prompt    string                   `json:"prompt"`\n    Messages  []map[string]interface{} `json:"messages,omitempty"`\n    Model     string                   `json:"model,omitempty"`\n    MaxTokens int                      `json:"max_tokens,omitempty"`\n    Params    map[string]any           `json:"params,omitempty"`\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"response-structure",children:"Response Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-go",children:'type LLMResponse struct {\n    Text         string `json:"text"`\n    InputTokens  int    `json:"input_tokens"`\n    OutputTokens int    `json:"output_tokens"`\n    TokensUsed   int    `json:"tokens_used"`\n    FinishReason string `json:"finish_reason"`\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"configuration-structure",children:"Configuration Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-go",children:'type LLMConfig struct {\n    Type    ProviderType `yaml:"type"`\n    APIKeys []string     `yaml:"api_keys"`\n    BaseURL string       `yaml:"base_url,omitempty"`\n    Model   string       `yaml:"model"`\n    Pricing Pricing      `yaml:"pricing"`\n    Limits  Limits       `yaml:"limits"`\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"adding-custom-providers",children:"Adding Custom Providers"}),"\n",(0,s.jsxs)(n.p,{children:["To add a new provider, implement the ",(0,s.jsx)(n.code,{children:"LLMProvider"})," interface and register it in the registry."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsxs)(n.strong,{children:["Create a new provider file (e.g., ",(0,s.jsx)(n.code,{children:"internal/provider/custom.go"}),"):"]})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-go",children:'package provider\n\nimport (\n    "bytes"\n    "context"\n    "encoding/json"\n    "fmt"\n    "io"\n    "net/http"\n    "time"\n)\n\ntype CustomProvider struct {\n    config LLMConfig\n}\n\nfunc NewCustomProvider(config LLMConfig) LLMProvider {\n    return &CustomProvider{config: config}\n}\n\nfunc (p *CustomProvider) Name() string {\n    return "custom" // Provider type identifier\n}\n\nfunc (p *CustomProvider) Generate(ctx context.Context, req *LLMRequest) (*LLMResponse, error) {\n    // Get API key (round-robin if multiple)\n    apiKey := p.config.APIKey()\n    if apiKey == "" {\n        return nil, fmt.Errorf("no API key available")\n    }\n\n    // Transform to provider-specific request format\n    providerReq := map[string]interface{}{\n        "model": req.Model,\n        "messages": req.Messages,\n        "max_tokens": req.MaxTokens,\n    }\n\n    // Make HTTP request\n    jsonData, _ := json.Marshal(providerReq)\n    httpReq, err := http.NewRequestWithContext(ctx, "POST",\n        p.config.BaseURL+"/chat/completions", bytes.NewBuffer(jsonData))\n    if err != nil {\n        return nil, err\n    }\n\n    httpReq.Header.Set("Content-Type", "application/json")\n    httpReq.Header.Set("Authorization", "Bearer "+apiKey)\n\n    client := &http.Client{Timeout: 30 * time.Second}\n    resp, err := client.Do(httpReq)\n    if err != nil {\n        return nil, err\n    }\n    defer resp.Body.Close()\n\n    // Read response\n    body, err := io.ReadAll(resp.Body)\n    if err != nil {\n        return nil, err\n    }\n\n    if resp.StatusCode != 200 {\n        return nil, fmt.Errorf("provider error: %s", string(body))\n    }\n\n    // Parse provider response\n    var providerResp map[string]interface{}\n    if err := json.Unmarshal(body, &providerResp); err != nil {\n        return nil, err\n    }\n\n    // Extract response data\n    choices := providerResp["choices"].([]interface{})\n    if len(choices) == 0 {\n        return nil, fmt.Errorf("no choices in response")\n    }\n\n    choice := choices[0].(map[string]interface{})\n    message := choice["message"].(map[string]interface{})\n    text := message["content"].(string)\n\n    // Extract usage (adjust based on provider\'s response format)\n    usage := providerResp["usage"].(map[string]interface{})\n    inputTokens := int(usage["prompt_tokens"].(float64))\n    outputTokens := int(usage["completion_tokens"].(float64))\n\n    return &LLMResponse{\n        Text:         text,\n        InputTokens:  inputTokens,\n        OutputTokens: outputTokens,\n        TokensUsed:   inputTokens + outputTokens,\n        FinishReason: choice["finish_reason"].(string),\n    }, nil\n}\n\nfunc (p *CustomProvider) ListModels(ctx context.Context) ([]string, error) {\n    // Return supported models for this provider\n    return []string{p.config.Model}, nil\n}\n'})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsxs)(n.strong,{children:["Register the provider in ",(0,s.jsx)(n.code,{children:"internal/provider/registry.go"}),":"]})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-go",children:'func (r *Registry) LoadFromConfig(cfg *config.Config) error {\n    // ... existing code for LLMProviders ...\n\n    for _, lp := range cfg.LLMProviders {\n        llmCfg := LLMConfig{\n            Type:    ProviderType(lp.Type),\n            APIKeys: lp.APIKeys,\n            BaseURL: lp.BaseURL,\n            Model:   lp.Model,\n            Pricing: lp.Pricing,\n            Limits:  lp.Limits,\n        }\n        var p LLMProvider\n        switch lp.Type {\n        case "openai":\n            p = NewOpenAIProvider(llmCfg)\n        case "gemini":\n            p = NewGeminiProvider(llmCfg)\n        case "claude":\n            p = NewClaudeProvider(llmCfg)\n        case "custom":\n            p = NewCustomProvider(llmCfg)\n        default:\n            return fmt.Errorf("unsupported provider type: %s", lp.Type)\n        }\n        r.Register(p)\n    }\n    return nil\n}\n'})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Add to configuration:"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'llm_providers:\n  - id: "my-custom"\n    type: "custom"\n    api_keys: ["${CUSTOM_API_KEY}"]\n    base_url: "https://api.my-custom-provider.com/v1"\n    model: "my-model"\n    pricing:\n      input_token_cost: 0.001\n      output_token_cost: 0.002\n    limits:\n      req_per_min: 100\n      tokens_per_min: 50000\n\nmodel_aliases:\n  my-model: my-custom:my-model\n'})}),"\n",(0,s.jsx)(n.h2,{id:"provider-specific-features",children:"Provider-Specific Features"}),"\n",(0,s.jsx)(n.h3,{id:"openai-features",children:"OpenAI Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Streaming responses"}),"\n",(0,s.jsx)(n.li,{children:"Function calling"}),"\n",(0,s.jsx)(n.li,{children:"Vision models"}),"\n",(0,s.jsx)(n.li,{children:"Fine-tuned models"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"gemini-features",children:"Gemini Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Multimodal inputs (text, images)"}),"\n",(0,s.jsx)(n.li,{children:"Function calling"}),"\n",(0,s.jsx)(n.li,{children:"Grounding with Google Search"}),"\n",(0,s.jsx)(n.li,{children:"Code execution"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"claude-features",children:"Claude Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Large context windows (200K tokens)"}),"\n",(0,s.jsx)(n.li,{children:"Advanced reasoning"}),"\n",(0,s.jsx)(n.li,{children:"Constitutional AI safety"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"pricing-integration",children:"Pricing Integration"}),"\n",(0,s.jsx)(n.p,{children:"Each provider includes real-time pricing information:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input Token Cost:"})," Cost per 1,000 input tokens"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Output Token Cost:"})," Cost per 1,000 output tokens"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Currency:"})," Billing currency (typically USD)"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Pricing data is used for cost optimization in load balancing."}),"\n",(0,s.jsx)(n.h2,{id:"rate-limiting",children:"Rate Limiting"}),"\n",(0,s.jsx)(n.p,{children:"Each key has configurable rate limits:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Requests per Minute:"})," Maximum API calls per minute"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tokens per Minute:"})," Maximum tokens processed per minute"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"COO-LLM automatically rotates keys when limits are approached."}),"\n",(0,s.jsx)(n.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,s.jsx)(n.p,{children:"Providers handle various error conditions:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rate Limits (429):"})," Automatic retry with backoff"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Authentication Errors (401/403):"})," Key rotation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Server Errors (5xx):"})," Failover to alternative providers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Not Found (404):"})," Fallback model selection"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"monitoring",children:"Monitoring"}),"\n",(0,s.jsx)(n.p,{children:"Provider performance is tracked:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Request latency"}),"\n",(0,s.jsx)(n.li,{children:"Success/error rates"}),"\n",(0,s.jsx)(n.li,{children:"Token usage"}),"\n",(0,s.jsx)(n.li,{children:"Cost accumulation"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Metrics are exposed via Prometheus and admin APIs."}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"key-management",children:"Key Management"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use multiple API keys per provider for redundancy"}),"\n",(0,s.jsx)(n.li,{children:"Rotate keys regularly for security"}),"\n",(0,s.jsx)(n.li,{children:"Monitor key usage and limits"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"cost-optimization",children:"Cost Optimization"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Configure accurate pricing information"}),"\n",(0,s.jsx)(n.li,{children:"Use cost-first strategies for budget-conscious deployments"}),"\n",(0,s.jsx)(n.li,{children:"Monitor spending via admin APIs"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"reliability",children:"Reliability"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Configure multiple providers for failover"}),"\n",(0,s.jsx)(n.li,{children:"Set appropriate rate limits"}),"\n",(0,s.jsx)(n.li,{children:"Monitor error rates and latency"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"security",children:"Security"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Store API keys securely (environment variables)"}),"\n",(0,s.jsx)(n.li,{children:"Use HTTPS for all provider communications"}),"\n",(0,s.jsx)(n.li,{children:"Implement proper authentication and authorization"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var r=i(6540);const s={},t=r.createContext(s);function o(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);