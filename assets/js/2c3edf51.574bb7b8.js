"use strict";(globalThis.webpackChunkcoo_llm_docs=globalThis.webpackChunkcoo_llm_docs||[]).push([[978],{7461:e=>{e.exports=JSON.parse('{"label":"user-guide","permalink":"/coo-llm-main/docs/tags/user-guide","allTagsPath":"/coo-llm-main/docs/tags","count":4,"items":[{"id":"Guides/Configuration","title":"Configuration","description":"COO-LLM uses YAML configuration files for all settings. The configuration is hierarchical and supports environment variable substitution, validation, and hot-reload.","permalink":"/coo-llm-main/docs/Guides/Configuration"},{"id":"Intro/Overview","title":"COO-LLM Overview","description":"COO-LLM is an intelligent reverse proxy and load balancer for Large Language Model (LLM) APIs. It provides a unified, OpenAI-compatible interface to multiple LLM providers while intelligently distributing requests across API keys and providers based on performance, cost, and rate limits.","permalink":"/coo-llm-main/docs/Intro/Overview"},{"id":"Guides/Deployment","title":"Deployment","description":"This guide covers deploying COO-LLM in various environments, from development to production.","permalink":"/coo-llm-main/docs/Guides/Deployment"},{"id":"Guides/Providers","title":"Providers","description":"COO-LLM supports multiple LLM providers through a plugin-based architecture. Each provider implements a common interface for seamless integration.","permalink":"/coo-llm-main/docs/Guides/Providers"}]}')}}]);