"use strict";(globalThis.webpackChunkcoo_llm_docs=globalThis.webpackChunkcoo_llm_docs||[]).push([[414],{1672:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>p});var a=t(8168),i=(t(6540),t(5680));const r={sidebar_position:2,tags:["developer-guide","api"]},l="API Reference",o={unversionedId:"Reference/API",id:"Reference/API",title:"API Reference",description:"COO-LLM provides OpenAI-compatible REST APIs for LLM interactions, plus administrative endpoints for management.",source:"@site/docs/Reference/API.md",sourceDirName:"Reference",slug:"/Reference/API",permalink:"/docs/docs/Reference/API",draft:!1,editUrl:"https://github.com/your-org/coo-llm/tree/main/docs/docs/docs/Reference/API.md",tags:[{label:"developer-guide",permalink:"/docs/docs/tags/developer-guide"},{label:"api",permalink:"/docs/docs/tags/api"}],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,tags:["developer-guide","api"]},sidebar:"tutorialSidebar",previous:{title:"Architecture",permalink:"/docs/docs/Intro/Architecture"},next:{title:"Load Balancer",permalink:"/docs/docs/Reference/Balancer"}},s={},p=[{value:"Authentication",id:"authentication",level:2},{value:"OpenAI-Compatible Endpoints",id:"openai-compatible-endpoints",level:2},{value:"POST /v1/chat/completions",id:"post-v1chatcompletions",level:3},{value:"GET /v1/models",id:"get-v1models",level:3},{value:"Admin API Endpoints",id:"admin-api-endpoints",level:2},{value:"Planned Admin Endpoints",id:"planned-admin-endpoints",level:3},{value:"Current Admin Access",id:"current-admin-access",level:3},{value:"Metrics Endpoint",id:"metrics-endpoint",level:2},{value:"GET /metrics",id:"get-metrics",level:3},{value:"Error Responses",id:"error-responses",level:2},{value:"Rate Limiting",id:"rate-limiting",level:2},{value:"Streaming",id:"streaming",level:2},{value:"Examples",id:"examples",level:2},{value:"Python Client",id:"python-client",level:3},{value:"cURL Examples",id:"curl-examples",level:3},{value:"Authentication",id:"authentication-1",level:2},{value:"SDK Compatibility",id:"sdk-compatibility",level:2}],d={toc:p},m="wrapper";function u({components:e,...n}){return(0,i.yg)(m,(0,a.A)({},d,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"api-reference"},"API Reference"),(0,i.yg)("p",null,"COO-LLM provides OpenAI-compatible REST APIs for LLM interactions, plus administrative endpoints for management."),(0,i.yg)("h2",{id:"authentication"},"Authentication"),(0,i.yg)("p",null,"All API requests require authentication via Bearer token:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre"},"Authorization: Bearer <api_key>\n")),(0,i.yg)("p",null,"API keys are configured in the providers section and mapped to specific keys."),(0,i.yg)("h2",{id:"openai-compatible-endpoints"},"OpenAI-Compatible Endpoints"),(0,i.yg)("h3",{id:"post-v1chatcompletions"},"POST /v1/chat/completions"),(0,i.yg)("p",null,"Generate chat completions using available models. This is the primary endpoint implemented in COO-LLM."),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Request Body:")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "model": "gpt-4o",\n  "messages": [\n    {\n      "role": "user",\n      "content": "Hello, how are you?"\n    }\n  ],\n  "max_tokens": 100\n}\n')),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Response:")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "id": "chatcmpl-1234567890",\n  "object": "chat.completion",\n  "created": 1699123456,\n  "model": "gpt-4o",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "Hello! I\'m doing well, thank you for asking."\n      },\n      "finish_reason": "stop"\n    }\n  ],\n  "usage": {\n    "prompt_tokens": 13,\n    "completion_tokens": 7,\n    "total_tokens": 20\n  }\n}\n')),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Parameters:")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"model"),' (string, required): Model alias from configuration (e.g., "gpt-4o", "gemini-1.5-pro")'),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"messages")," (array, required): Chat messages with role/content format"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"max_tokens")," (integer, optional): Maximum tokens to generate (default: 1000)"),(0,i.yg)("li",{parentName:"ul"},"Additional parameters are passed through to the provider")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Features:")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"\u2705 Conversation history support"),(0,i.yg)("li",{parentName:"ul"},"\u2705 Model alias resolution"),(0,i.yg)("li",{parentName:"ul"},"\u2705 Automatic provider selection and load balancing"),(0,i.yg)("li",{parentName:"ul"},"\u2705 Rate limiting and retry logic"),(0,i.yg)("li",{parentName:"ul"},"\u2705 Response caching (if enabled)"),(0,i.yg)("li",{parentName:"ul"},"\u2705 Usage tracking and metrics")),(0,i.yg)("h3",{id:"get-v1models"},"GET /v1/models"),(0,i.yg)("p",null,"List available models based on configured model aliases."),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Response:")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "object": "list",\n  "data": [\n    {\n      "id": "gpt-4o",\n      "object": "model",\n      "created": 1699123456,\n      "owned_by": "coo-llm"\n    },\n    {\n      "id": "gemini-1.5-pro",\n      "object": "model",\n      "created": 1699123456,\n      "owned_by": "coo-llm"\n    }\n  ]\n}\n')),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Note:")," Models are listed based on ",(0,i.yg)("inlineCode",{parentName:"p"},"model_aliases")," configuration, not actual provider models."),(0,i.yg)("h2",{id:"admin-api-endpoints"},"Admin API Endpoints"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Note:")," Admin API endpoints are not yet implemented in the current version. The following are planned for future releases:"),(0,i.yg)("h3",{id:"planned-admin-endpoints"},"Planned Admin Endpoints"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"GET /admin/v1/config")," - Get current configuration"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"POST /admin/v1/config")," - Update configuration"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"POST /admin/v1/config/validate")," - Validate configuration"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"POST /admin/v1/reload")," - Reload configuration from file"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"GET /admin/v1/providers")," - Get provider status and metrics"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"GET /admin/v1/logs")," - Get recent log entries")),(0,i.yg)("h3",{id:"current-admin-access"},"Current Admin Access"),(0,i.yg)("p",null,"Currently, configuration management is done via:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Configuration file reloading (restart required)"),(0,i.yg)("li",{parentName:"ul"},"Direct file editing"),(0,i.yg)("li",{parentName:"ul"},"Environment variable changes")),(0,i.yg)("p",null,"Admin functionality will be added in future versions."),(0,i.yg)("h2",{id:"metrics-endpoint"},"Metrics Endpoint"),(0,i.yg)("h3",{id:"get-metrics"},"GET /metrics"),(0,i.yg)("p",null,"Prometheus metrics endpoint (enabled when ",(0,i.yg)("inlineCode",{parentName:"p"},"logging.prometheus.enabled: true"),")."),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Response:")," Prometheus format metrics"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre"},'# HELP llm_requests_total Total number of LLM requests\n# TYPE llm_requests_total counter\nllm_requests_total{provider="openai",model="gpt-4"} 1250\n\n# HELP llm_request_duration_seconds Request duration in seconds\n# TYPE llm_request_duration_seconds histogram\nllm_request_duration_seconds_bucket{provider="openai",le="0.1"} 1200\n...\n')),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Available Metrics:")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Request counts by provider/model"),(0,i.yg)("li",{parentName:"ul"},"Request duration histograms"),(0,i.yg)("li",{parentName:"ul"},"Error rates"),(0,i.yg)("li",{parentName:"ul"},"Token usage tracking"),(0,i.yg)("li",{parentName:"ul"},"Active connections")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Configuration:")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},'logging:\n  prometheus:\n    enabled: true\n    endpoint: "/metrics"\n')),(0,i.yg)("h2",{id:"error-responses"},"Error Responses"),(0,i.yg)("p",null,"All endpoints return standard HTTP status codes:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"200"),": Success"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"400"),": Bad Request (invalid parameters)"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"401"),": Unauthorized (invalid API key)"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"403"),": Forbidden (rate limited or quota exceeded)"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"404"),": Not Found (invalid endpoint or model)"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"429"),": Too Many Requests (rate limited)"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"500"),": Internal Server Error"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"502"),": Bad Gateway (provider error)"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"503"),": Service Unavailable (provider down)")),(0,i.yg)("p",null,"Error response format:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "error": {\n    "message": "Invalid model specified",\n    "type": "invalid_request_error",\n    "code": 400\n  }\n}\n')),(0,i.yg)("h2",{id:"rate-limiting"},"Rate Limiting"),(0,i.yg)("p",null,"COO-LLM implements rate limiting based on configured limits:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Per-key request limits"),(0,i.yg)("li",{parentName:"ul"},"Per-key token limits"),(0,i.yg)("li",{parentName:"ul"},"Global rate limits")),(0,i.yg)("p",null,"Rate limited requests return ",(0,i.yg)("inlineCode",{parentName:"p"},"429")," status with retry information."),(0,i.yg)("h2",{id:"streaming"},"Streaming"),(0,i.yg)("p",null,"Streaming responses are supported for chat completions:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},'curl -X POST http://localhost:8080/v1/chat/completions \\\n  -H "Authorization: Bearer your-key" \\\n  -d \'{"model": "gpt-4", "messages": [{"role": "user", "content": "Tell me a story"}], "stream": true}\'\n')),(0,i.yg)("p",null,"Response is Server-Sent Events format."),(0,i.yg)("h2",{id:"examples"},"Examples"),(0,i.yg)("h3",{id:"python-client"},"Python Client"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'import openai\n\n# Point to COO-LLM instead of OpenAI\nclient = openai.OpenAI(\n    api_key="dummy-key",  # COO-LLM ignores this, uses config-based auth\n    base_url="http://localhost:8080/v1"\n)\n\nresponse = client.chat.completions.create(\n    model="gpt-4o",\n    messages=[{"role": "user", "content": "Hello!"}]\n)\n\nprint(response.choices[0].message.content)\n')),(0,i.yg)("h3",{id:"curl-examples"},"cURL Examples"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},'# Chat completion with API key auth\ncurl -X POST http://localhost:8080/v1/chat/completions \\\n  -H "Authorization: Bearer test-key" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "gpt-4o",\n    "messages": [{"role": "user", "content": "Hello"}]\n  }\'\n\n# List available models\ncurl http://localhost:8080/v1/models \\\n  -H "Authorization: Bearer test-key"\n\n# Prometheus metrics\ncurl http://localhost:8080/metrics\n')),(0,i.yg)("h2",{id:"authentication-1"},"Authentication"),(0,i.yg)("p",null,"COO-LLM uses API key authentication via the ",(0,i.yg)("inlineCode",{parentName:"p"},"Authorization: Bearer <key>")," header. API keys are configured in the ",(0,i.yg)("inlineCode",{parentName:"p"},"api_keys")," section and map to allowed providers."),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},'api_keys:\n  - key: "client-a-key"\n    allowed_providers: ["openai-prod"]  # Limited access\n  - key: "premium-key"\n    allowed_providers: ["*"]  # Full access\n')),(0,i.yg)("h2",{id:"sdk-compatibility"},"SDK Compatibility"),(0,i.yg)("p",null,"COO-LLM is compatible with:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"OpenAI Python SDK (",(0,i.yg)("inlineCode",{parentName:"li"},"openai>=1.0"),")"),(0,i.yg)("li",{parentName:"ul"},"OpenAI Node.js SDK"),(0,i.yg)("li",{parentName:"ul"},"Any HTTP client following OpenAI Chat Completions API format")),(0,i.yg)("p",null,"Simply change the ",(0,i.yg)("inlineCode",{parentName:"p"},"base_url")," to point to your COO-LLM instance and use any API key from your configuration."))}u.isMDXComponent=!0},5680:(e,n,t)=>{t.d(n,{xA:()=>d,yg:()=>c});var a=t(6540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach(function(n){i(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function o(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=a.createContext({}),p=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},d=function(e){var n=p(e.components);return a.createElement(s.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},g=a.forwardRef(function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),m=p(t),g=i,c=m["".concat(s,".").concat(g)]||m[g]||u[g]||r;return t?a.createElement(c,l(l({ref:n},d),{},{components:t})):a.createElement(c,l({ref:n},d))});function c(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,l=new Array(r);l[0]=g;var o={};for(var s in n)hasOwnProperty.call(n,s)&&(o[s]=n[s]);o.originalType=e,o[m]="string"==typeof e?e:i,l[1]=o;for(var p=2;p<r;p++)l[p]=t[p];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}g.displayName="MDXCreateElement"}}]);