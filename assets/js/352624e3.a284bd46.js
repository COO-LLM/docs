"use strict";(globalThis.webpackChunkcoo_llm_docs=globalThis.webpackChunkcoo_llm_docs||[]).push([[5797],{8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>r});var t=s(6540);const o={},i=t.createContext(o);function a(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(i.Provider,{value:n},e.children)}},9869:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"User-Guide/Examples","title":"Examples","description":"Collection of code examples for common use cases with COO-LLM.","source":"@site/content/User-Guide/Examples.md","sourceDirName":"User-Guide","slug":"/User-Guide/Examples","permalink":"/docs/docs/User-Guide/Examples","draft":false,"unlisted":false,"editUrl":"https://github.com/coo-llm/coo-llm-main/tree/main/docs/content/content/User-Guide/Examples.md","tags":[{"inline":true,"label":"user-guide","permalink":"/docs/docs/tags/user-guide"},{"inline":true,"label":"examples","permalink":"/docs/docs/tags/examples"}],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"tags":["user-guide","examples"]},"sidebar":"tutorialSidebar","previous":{"title":"Providers","permalink":"/docs/docs/Guides/Providers"},"next":{"title":"Administrator Guide Overview","permalink":"/docs/docs/Administrator-Guide/Overview"}}');var o=s(4848),i=s(8453);const a={sidebar_position:3,tags:["user-guide","examples"]},r="Examples",l={},c=[{value:"Python",id:"python",level:2},{value:"Basic Chat Completion",id:"basic-chat-completion",level:3},{value:"Streaming Response",id:"streaming-response",level:3},{value:"Error Handling",id:"error-handling",level:3},{value:"JavaScript/Node.js",id:"javascriptnodejs",level:2},{value:"Basic Usage with Axios",id:"basic-usage-with-axios",level:3},{value:"Streaming with Fetch API",id:"streaming-with-fetch-api",level:3},{value:"cURL",id:"curl",level:2},{value:"Simple Request",id:"simple-request",level:3},{value:"With Custom Parameters",id:"with-custom-parameters",level:3},{value:"Testing Rate Limits",id:"testing-rate-limits",level:3},{value:"Advanced Examples",id:"advanced-examples",level:2},{value:"Multi-Provider Fallback",id:"multi-provider-fallback",level:3},{value:"Cost Tracking",id:"cost-tracking",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"examples",children:"Examples"})}),"\n",(0,o.jsx)(n.p,{children:"Collection of code examples for common use cases with COO-LLM."}),"\n",(0,o.jsx)(n.h2,{id:"python",children:"Python"}),"\n",(0,o.jsx)(n.h3,{id:"basic-chat-completion",children:"Basic Chat Completion"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import requests\n\nAPI_KEY = "your-api-key"\nBASE_URL = "http://localhost:2906/v1"\n\ndef chat_completion(model, messages):\n    response = requests.post(\n        f"{BASE_URL}/chat/completions",\n        headers={\n            "Authorization": f"Bearer {API_KEY}",\n            "Content-Type": "application/json"\n        },\n        json={\n            "model": model,\n            "messages": messages,\n            "max_tokens": 1000,\n            "temperature": 0.7\n        }\n    )\n    return response.json()\n\n# Usage\nmessages = [\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "Explain quantum computing in simple terms."}\n]\n\nresult = chat_completion("openai:gpt-4o", messages)\nprint(result["choices"][0]["message"]["content"])\n'})}),"\n",(0,o.jsx)(n.h3,{id:"streaming-response",children:"Streaming Response"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import requests\nimport json\n\ndef stream_chat(model, messages):\n    response = requests.post(\n        f"{BASE_URL}/chat/completions",\n        headers={\n            "Authorization": f"Bearer {API_KEY}",\n            "Content-Type": "application/json"\n        },\n        json={\n            "model": model,\n            "messages": messages,\n            "stream": True\n        },\n        stream=True\n    )\n\n    for line in response.iter_lines():\n        if line:\n            line = line.decode(\'utf-8\')\n            if line.startswith(\'data: \'):\n                data = line[6:]\n                if data == \'[DONE]\':\n                    break\n                chunk = json.loads(data)\n                content = chunk["choices"][0]["delta"].get("content", "")\n                print(content, end="", flush=True)\n\n# Usage\nmessages = [{"role": "user", "content": "Write a short story about AI."}]\nstream_chat("gemini:gemini-1.5-pro", messages)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"error-handling",children:"Error Handling"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def safe_chat_completion(model, messages, retries=3):\n    for attempt in range(retries):\n        try:\n            response = requests.post(\n                f"{BASE_URL}/chat/completions",\n                headers={\n                    "Authorization": f"Bearer {API_KEY}",\n                    "Content-Type": "application/json"\n                },\n                json={\n                    "model": model,\n                    "messages": messages\n                },\n                timeout=30\n            )\n\n            if response.status_code == 429:\n                # Rate limited, wait and retry\n                wait_time = 2 ** attempt  # Exponential backoff\n                print(f"Rate limited, waiting {wait_time}s...")\n                time.sleep(wait_time)\n                continue\n            elif response.status_code != 200:\n                raise Exception(f"API error: {response.status_code} - {response.text}")\n\n            return response.json()\n\n        except requests.exceptions.RequestException as e:\n            print(f"Request failed: {e}")\n            if attempt < retries - 1:\n                time.sleep(1)\n            continue\n\n    raise Exception("All retries failed")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"javascriptnodejs",children:"JavaScript/Node.js"}),"\n",(0,o.jsx)(n.h3,{id:"basic-usage-with-axios",children:"Basic Usage with Axios"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"const axios = require('axios');\n\nconst API_KEY = 'your-api-key';\nconst BASE_URL = 'http://localhost:2906/v1';\n\nasync function chatCompletion(model, messages) {\n  try {\n    const response = await axios.post(`${BASE_URL}/chat/completions`, {\n      model: model,\n      messages: messages,\n      max_tokens: 1000,\n      temperature: 0.7\n    }, {\n      headers: {\n        'Authorization': `Bearer ${API_KEY}`,\n        'Content-Type': 'application/json'\n      }\n    });\n\n    return response.data;\n  } catch (error) {\n    console.error('Error:', error.response?.data || error.message);\n    throw error;\n  }\n}\n\n// Usage\nconst messages = [\n  { role: 'system', content: 'You are a coding assistant.' },\n  { role: 'user', content: 'Write a Python function to reverse a string.' }\n];\n\nchatCompletion('openai:gpt-4o', messages)\n  .then(result => {\n    console.log(result.choices[0].message.content);\n  });\n"})}),"\n",(0,o.jsx)(n.h3,{id:"streaming-with-fetch-api",children:"Streaming with Fetch API"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-javascript",children:"async function streamChat(model, messages) {\n  const response = await fetch(`${BASE_URL}/chat/completions`, {\n    method: 'POST',\n    headers: {\n      'Authorization': `Bearer ${API_KEY}`,\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: model,\n      messages: messages,\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n');\n\n    for (const line of lines) {\n      if (line.startsWith('data: ')) {\n        const data = line.slice(6);\n        if (data === '[DONE]') return;\n\n        try {\n          const parsed = JSON.parse(data);\n          const content = parsed.choices[0]?.delta?.content || '';\n          process.stdout.write(content);\n        } catch (e) {\n          // Ignore parse errors\n        }\n      }\n    }\n  }\n}\n"})}),"\n",(0,o.jsx)(n.h2,{id:"curl",children:"cURL"}),"\n",(0,o.jsx)(n.h3,{id:"simple-request",children:"Simple Request"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:2906/api/v1/chat/completions \\\n  -H "Authorization: Bearer your-key" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "openai:gpt-4o",\n    "messages": [{"role": "user", "content": "Hello!"}]\n  }\'\n'})}),"\n",(0,o.jsx)(n.h3,{id:"with-custom-parameters",children:"With Custom Parameters"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:2906/api/v1/chat/completions \\\n  -H "Authorization: Bearer your-key" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "claude:claude-3-opus-20240229",\n    "messages": [\n      {"role": "system", "content": "Be concise."},\n      {"role": "user", "content": "Explain recursion."}\n    ],\n    "max_tokens": 500,\n    "temperature": 0.3\n  }\'\n'})}),"\n",(0,o.jsx)(n.h3,{id:"testing-rate-limits",children:"Testing Rate Limits"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:'# Send multiple requests quickly\nfor i in {1..10}; do\n  curl -X POST http://localhost:2906/api/v1/chat/completions \\\n    -H "Authorization: Bearer your-key" \\\n    -H "Content-Type: application/json" \\\n    -d \'{"model": "openai:gpt-4o", "messages": [{"role": "user", "content": "Hi"}]}\' \\\n    -w "Status: %{http_code}\\n" -o /dev/null &\ndone\nwait\n'})}),"\n",(0,o.jsx)(n.h2,{id:"advanced-examples",children:"Advanced Examples"}),"\n",(0,o.jsx)(n.h3,{id:"multi-provider-fallback",children:"Multi-Provider Fallback"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def smart_completion(messages, preferred_providers=['openai', 'gemini', 'claude']):\n    for provider in preferred_providers:\n        try:\n            model = f\"{provider}:gpt-4o\" if provider == 'openai' else f\"{provider}:gemini-1.5-pro\" if provider == 'gemini' else f\"{provider}:claude-3-opus-20240229\"\n            return chat_completion(model, messages)\n        except Exception as e:\n            print(f\"{provider} failed: {e}\")\n            continue\n    raise Exception(\"All providers failed\")\n"})}),"\n",(0,o.jsx)(n.h3,{id:"cost-tracking",children:"Cost Tracking"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def completion_with_cost_tracking(model, messages):\n    start_time = time.time()\n    result = chat_completion(model, messages)\n    end_time = time.time()\n\n    usage = result.get('usage', {})\n    prompt_tokens = usage.get('prompt_tokens', 0)\n    completion_tokens = usage.get('completion_tokens', 0)\n\n    # Estimate cost (simplified)\n    cost_per_token = 0.002 / 1000  # Example for GPT-4\n    estimated_cost = (prompt_tokens + completion_tokens) * cost_per_token\n\n    return {\n        'response': result,\n        'cost': estimated_cost,\n        'latency': end_time - start_time\n    }\n"})})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);