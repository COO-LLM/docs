"use strict";(globalThis.webpackChunkcoo_llm_docs=globalThis.webpackChunkcoo_llm_docs||[]).push([[3357],{5203:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>t,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"Reference/API","title":"API Reference","description":"COO-LLM provides OpenAI-compatible REST APIs for LLM interactions, plus administrative endpoints for management.","source":"@site/content/Reference/API.md","sourceDirName":"Reference","slug":"/Reference/API","permalink":"/docs/docs/Reference/API","draft":false,"unlisted":false,"editUrl":"https://github.com/coo-llm/coo-llm-main/tree/main/docs/content/content/Reference/API.md","tags":[{"inline":true,"label":"developer-guide","permalink":"/docs/docs/tags/developer-guide"},{"inline":true,"label":"api","permalink":"/docs/docs/tags/api"}],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"tags":["developer-guide","api"]},"sidebar":"tutorialSidebar","previous":{"title":"Architecture","permalink":"/docs/docs/Intro/Architecture"},"next":{"title":"Load Balancer","permalink":"/docs/docs/Reference/Balancer"}}');var r=i(4848),o=i(8453);const l={sidebar_position:2,tags:["developer-guide","api"]},t="API Reference",d={},c=[{value:"Authentication",id:"authentication",level:2},{value:"Model Resolution",id:"model-resolution",level:2},{value:"Authentication &amp; Authorization Flow",id:"authentication--authorization-flow",level:2},{value:"API Request Flow",id:"api-request-flow",level:2},{value:"OpenAI-Compatible Endpoints",id:"openai-compatible-endpoints",level:2},{value:"POST /v1/chat/completions",id:"post-v1chatcompletions",level:3},{value:"GET /v1/models",id:"get-v1models",level:3},{value:"Admin API Endpoints",id:"admin-api-endpoints",level:2},{value:"Planned Admin Endpoints",id:"planned-admin-endpoints",level:3},{value:"Current Admin Access",id:"current-admin-access",level:3},{value:"Metrics Endpoint",id:"metrics-endpoint",level:2},{value:"GET /metrics",id:"get-metrics",level:3},{value:"Error Responses",id:"error-responses",level:2},{value:"Rate Limiting",id:"rate-limiting",level:2},{value:"Streaming",id:"streaming",level:2},{value:"Examples",id:"examples",level:2},{value:"Python Client",id:"python-client",level:3},{value:"cURL Examples",id:"curl-examples",level:3},{value:"Authentication",id:"authentication-1",level:2},{value:"SDK Compatibility",id:"sdk-compatibility",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"api-reference",children:"API Reference"})}),"\n",(0,r.jsx)(n.p,{children:"COO-LLM provides OpenAI-compatible REST APIs for LLM interactions, plus administrative endpoints for management."}),"\n",(0,r.jsx)(n.h2,{id:"authentication",children:"Authentication"}),"\n",(0,r.jsx)(n.p,{children:"All API requests require authentication via Bearer token:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Authorization: Bearer <api_key>\n"})}),"\n",(0,r.jsx)(n.p,{children:"API keys are configured in the providers section and mapped to specific keys."}),"\n",(0,r.jsx)(n.h2,{id:"model-resolution",children:"Model Resolution"}),"\n",(0,r.jsx)(n.p,{children:"COO-LLM supports 3 ways to specify models:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:["Direct provider",":model"," syntax"]}),": ",(0,r.jsx)(n.code,{children:"provider_id:model_name"})," (e.g., ",(0,r.jsx)(n.code,{children:"openai:gpt-4o"}),", ",(0,r.jsx)(n.code,{children:"custom:my-model"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model aliases"}),": Short names mapped to provider",":model"," (e.g., ",(0,r.jsx)(n.code,{children:"gpt-4o"})," \u2192 ",(0,r.jsx)(n.code,{children:"openai:gpt-4o"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pattern matching fallback"}),": Infer provider from model name (e.g., ",(0,r.jsx)(n.code,{children:"gpt-4o"})," \u2192 OpenAI provider)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"authentication--authorization-flow",children:"Authentication & Authorization Flow"}),"\n",(0,r.jsx)(n.mermaid,{value:"flowchart TD\n    classDef client fill:#28a745,color:#fff,stroke:#fff,stroke-width:2px\n    classDef auth fill:#ffc107,color:#000,stroke:#000,stroke-width:2px\n    classDef process fill:#dc3545,color:#fff,stroke:#fff,stroke-width:2px\n    classDef deny fill:#dc3545,color:#fff,stroke:#fff,stroke-width:2px\n\n    A[Client Request<br/>with Bearer token]:::client\n    A --\x3e B[Extract API Key<br/>from Authorization header]:::auth\n    B --\x3e C{Key exists in<br/>api_keys config?}:::auth\n\n    C --\x3e|No| D[401 Unauthorized<br/>Invalid API key]:::deny\n    C --\x3e|Yes| E[Get allowed_providers<br/>for this key]:::auth\n    E --\x3e F{Provider allowed<br/>for this request?}:::auth\n\n    F --\x3e|No| G[403 Forbidden<br/>Access denied to provider]:::deny\n    F --\x3e|Yes| H[Proceed to<br/>Request Processing]:::process"}),"\n",(0,r.jsx)(n.h2,{id:"api-request-flow",children:"API Request Flow"}),"\n",(0,r.jsx)(n.mermaid,{value:"flowchart TD\n    classDef client fill:#28a745,color:#fff,stroke:#fff,stroke-width:2px\n    classDef process fill:#dc3545,color:#fff,stroke:#fff,stroke-width:2px\n    classDef external fill:#007bff,color:#fff,stroke:#fff,stroke-width:2px\n\n    A[Client Request<br/>POST /v1/chat/completions]:::client\n    A --\x3e B[Authentication<br/>Bearer Token Validation]:::process\n    B --\x3e C[Model Alias Resolution<br/>Map to provider:model]:::process\n    C --\x3e D[Provider & Key Selection<br/>Load Balancing Algorithm]:::process\n    D --\x3e E[Rate Limit Check<br/>Per-key limits]:::process\n    E --\x3e F[External API Call<br/>OpenAI/Gemini/Claude]:::external\n    F --\x3e G[Response Processing<br/>Token counting, caching]:::process\n    G --\x3e H[Usage Tracking<br/>Metrics update]:::process\n    H --\x3e I[Return OpenAI-compatible<br/>JSON Response]:::client\n\n    B --\x3e J[401 Unauthorized]:::process\n    E --\x3e K[429 Rate Limited]:::process\n    F --\x3e L[Retry Logic<br/>Up to max_attempts]:::process\n    L --\x3e F\n    L --\x3e M[502/503 Error<br/>Provider failure]:::process"}),"\n",(0,r.jsx)(n.h2,{id:"openai-compatible-endpoints",children:"OpenAI-Compatible Endpoints"}),"\n",(0,r.jsx)(n.h3,{id:"post-v1chatcompletions",children:"POST /v1/chat/completions"}),"\n",(0,r.jsx)(n.p,{children:"Generate chat completions using available models. This is the primary endpoint implemented in COO-LLM."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Request Body:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "model": "gpt-4o",\n  "messages": [\n    {\n      "role": "user",\n      "content": "Hello, how are you?"\n    }\n  ],\n  "max_tokens": 100\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Response:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "id": "chatcmpl-1234567890",\n  "object": "chat.completion",\n  "created": 1699123456,\n  "model": "gpt-4o",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "Hello! I\'m doing well, thank you for asking."\n      },\n      "finish_reason": "stop"\n    }\n  ],\n  "usage": {\n    "prompt_tokens": 13,\n    "completion_tokens": 7,\n    "total_tokens": 20,\n    "cost": 0.000036\n  }\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"model"}),' (string, required): Model alias from configuration (e.g., "gpt-4o", "gemini-1.5-pro")']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"messages"})," (array, required): Chat messages with role/content format"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"max_tokens"})," (integer, optional): Maximum tokens to generate (default: 1000)"]}),"\n",(0,r.jsx)(n.li,{children:"Additional parameters are passed through to the provider"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Features:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Conversation history support"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Model alias resolution"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Automatic provider selection and load balancing"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Rate limiting and retry logic"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Response caching (if enabled)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Usage tracking and cost calculation"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Comprehensive logging"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"get-v1models",children:"GET /v1/models"}),"\n",(0,r.jsx)(n.p,{children:"List available models based on configured model aliases."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Response:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "object": "list",\n  "data": [\n    {\n      "id": "gpt-4o",\n      "object": "model",\n      "created": 1699123456,\n      "owned_by": "coo-llm"\n    },\n    {\n      "id": "gemini-1.5-pro",\n      "object": "model",\n      "created": 1699123456,\n      "owned_by": "coo-llm"\n    }\n  ]\n}\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," Models are listed based on ",(0,r.jsx)(n.code,{children:"model_aliases"})," configuration, not actual provider models."]}),"\n",(0,r.jsx)(n.h2,{id:"admin-api-endpoints",children:"Admin API Endpoints"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," Admin API endpoints are not yet implemented in the current version. The following are planned for future releases:"]}),"\n",(0,r.jsx)(n.h3,{id:"planned-admin-endpoints",children:"Planned Admin Endpoints"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"GET /admin/v1/config"})," - Get current configuration"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"POST /admin/v1/config"})," - Update configuration"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"POST /admin/v1/config/validate"})," - Validate configuration"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"POST /admin/v1/reload"})," - Reload configuration from file"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"GET /admin/v1/providers"})," - Get provider status and metrics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"GET /admin/v1/logs"})," - Get recent log entries"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"current-admin-access",children:"Current Admin Access"}),"\n",(0,r.jsx)(n.p,{children:"Currently, configuration management is done via:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Configuration file reloading (restart required)"}),"\n",(0,r.jsx)(n.li,{children:"Direct file editing"}),"\n",(0,r.jsx)(n.li,{children:"Environment variable changes"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Admin functionality will be added in future versions."}),"\n",(0,r.jsx)(n.h2,{id:"metrics-endpoint",children:"Metrics Endpoint"}),"\n",(0,r.jsx)(n.h3,{id:"get-metrics",children:"GET /metrics"}),"\n",(0,r.jsxs)(n.p,{children:["Prometheus metrics endpoint (enabled when ",(0,r.jsx)(n.code,{children:"logging.prometheus.enabled: true"}),")."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Response:"})," Prometheus format metrics"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'# HELP llm_requests_total Total number of LLM requests\n# TYPE llm_requests_total counter\nllm_requests_total{provider="openai",model="gpt-4"} 1250\n\n# HELP llm_request_duration_seconds Request duration in seconds\n# TYPE llm_request_duration_seconds histogram\nllm_request_duration_seconds_bucket{provider="openai",le="0.1"} 1200\n...\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Available Metrics:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Request counts by provider/model"}),"\n",(0,r.jsx)(n.li,{children:"Request duration histograms"}),"\n",(0,r.jsx)(n.li,{children:"Error rates"}),"\n",(0,r.jsx)(n.li,{children:"Token usage tracking"}),"\n",(0,r.jsx)(n.li,{children:"Active connections"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Configuration:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'logging:\n  prometheus:\n    enabled: true\n    endpoint: "/metrics"\n'})}),"\n",(0,r.jsx)(n.h2,{id:"error-responses",children:"Error Responses"}),"\n",(0,r.jsx)(n.p,{children:"All endpoints return standard HTTP status codes:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"200"}),": Success"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"400"}),": Bad Request (invalid parameters)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"401"}),": Unauthorized (invalid API key)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"403"}),": Forbidden (rate limited or quota exceeded)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"404"}),": Not Found (invalid endpoint or model)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"429"}),": Too Many Requests (rate limited)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"500"}),": Internal Server Error"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"502"}),": Bad Gateway (provider error)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"503"}),": Service Unavailable (provider down)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Error response format:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "error": {\n    "message": "Invalid model specified",\n    "type": "invalid_request_error",\n    "code": 400\n  }\n}\n'})}),"\n",(0,r.jsx)(n.h2,{id:"rate-limiting",children:"Rate Limiting"}),"\n",(0,r.jsx)(n.p,{children:"COO-LLM implements rate limiting based on configured limits:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Per-key request limits"}),"\n",(0,r.jsx)(n.li,{children:"Per-key token limits"}),"\n",(0,r.jsx)(n.li,{children:"Global rate limits"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Rate limited requests return ",(0,r.jsx)(n.code,{children:"429"})," status with retry information."]}),"\n",(0,r.jsx)(n.h2,{id:"streaming",children:"Streaming"}),"\n",(0,r.jsx)(n.p,{children:"Streaming responses are supported for chat completions:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:2906/v1/chat/completions \\\n  -H "Authorization: Bearer your-key" \\\n  -d \'{"model": "gpt-4", "messages": [{"role": "user", "content": "Tell me a story"}], "stream": true}\'\n'})}),"\n",(0,r.jsx)(n.p,{children:"Response is Server-Sent Events format."}),"\n",(0,r.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,r.jsx)(n.h3,{id:"python-client",children:"Python Client"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import openai\n\n# Point to COO-LLM instead of OpenAI\nclient = openai.OpenAI(\n    api_key="dummy-key",  # COO-LLM ignores this, uses config-based auth\n    base_url="http://localhost:2906/v1"\n)\n\nresponse = client.chat.completions.create(\n    model="gpt-4o",\n    messages=[{"role": "user", "content": "Hello!"}]\n)\n\nprint(response.choices[0].message.content)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"curl-examples",children:"cURL Examples"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Chat completion with API key auth\ncurl -X POST http://localhost:2906/v1/chat/completions \\\n  -H "Authorization: Bearer test-key" \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "gpt-4o",\n    "messages": [{"role": "user", "content": "Hello"}]\n  }\'\n\n# List available models\ncurl http://localhost:2906/v1/models \\\n  -H "Authorization: Bearer test-key"\n\n# Prometheus metrics\ncurl http://localhost:2906/metrics\n'})}),"\n",(0,r.jsx)(n.h2,{id:"authentication-1",children:"Authentication"}),"\n",(0,r.jsxs)(n.p,{children:["COO-LLM uses API key authentication via the ",(0,r.jsx)(n.code,{children:"Authorization: Bearer <key>"})," header. API keys are configured in the ",(0,r.jsx)(n.code,{children:"api_keys"})," section and map to allowed providers."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'api_keys:\n  - key: "client-a-key"\n    allowed_providers: ["openai-prod"]  # Limited access\n  - key: "premium-key"\n    allowed_providers: ["*"]  # Full access\n'})}),"\n",(0,r.jsx)(n.h2,{id:"sdk-compatibility",children:"SDK Compatibility"}),"\n",(0,r.jsx)(n.p,{children:"COO-LLM is compatible with:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["OpenAI Python SDK (",(0,r.jsx)(n.code,{children:"openai>=1.0"}),")"]}),"\n",(0,r.jsx)(n.li,{children:"OpenAI Node.js SDK"}),"\n",(0,r.jsx)(n.li,{children:"Any HTTP client following OpenAI Chat Completions API format"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Simply change the ",(0,r.jsx)(n.code,{children:"base_url"})," to point to your COO-LLM instance and use any API key from your configuration."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>t});var s=i(6540);const r={},o=s.createContext(r);function l(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);