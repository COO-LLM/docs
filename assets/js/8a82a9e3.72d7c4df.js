"use strict";(globalThis.webpackChunkcoo_llm_docs=globalThis.webpackChunkcoo_llm_docs||[]).push([[422],{5680:(e,t,r)=>{r.d(t,{xA:()=>u,yg:()=>m});var n=r(6540);function i(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function a(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter(function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable})),r.push.apply(r,n)}return r}function o(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?a(Object(r),!0).forEach(function(t){i(e,t,r[t])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach(function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))})}return e}function l(e,t){if(null==e)return{};var r,n,i=function(e,t){if(null==e)return{};var r,n,i={},a=Object.keys(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||(i[r]=e[r]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)r=a[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(i[r]=e[r])}return i}var s=n.createContext({}),p=function(e){var t=n.useContext(s),r=t;return e&&(r="function"==typeof e?e(t):o(o({},t),e)),r},u=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},c="mdxType",g={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef(function(e,t){var r=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),c=p(r),d=i,m=c["".concat(s,".").concat(d)]||c[d]||g[d]||a;return r?n.createElement(m,o(o({ref:t},u),{},{components:r})):n.createElement(m,o({ref:t},u))});function m(e,t){var r=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=r.length,o=new Array(a);o[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[c]="string"==typeof e?e:i,o[1]=l;for(var p=2;p<a;p++)o[p]=r[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,r)}d.displayName="MDXCreateElement"},9860:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>g,frontMatter:()=>a,metadata:()=>l,toc:()=>p});var n=r(8168),i=(r(6540),r(5680));const a={title:"COO-LLM Overview",sidebar_position:1,tags:["user-guide","getting-started"]},o=void 0,l={unversionedId:"Intro/Overview",id:"Intro/Overview",title:"COO-LLM Overview",description:"COO-LLM is an intelligent reverse proxy and load balancer for Large Language Model (LLM) APIs. It provides a unified, OpenAI-compatible interface to multiple LLM providers while intelligently distributing requests across API keys and providers based on performance, cost, and rate limits.",source:"@site/docs/Intro/Overview.md",sourceDirName:"Intro",slug:"/Intro/Overview",permalink:"/docs/docs/Intro/Overview",draft:!1,editUrl:"https://github.com/your-org/coo-llm/tree/main/docs/docs/docs/Intro/Overview.md",tags:[{label:"user-guide",permalink:"/docs/docs/tags/user-guide"},{label:"getting-started",permalink:"/docs/docs/tags/getting-started"}],version:"current",sidebarPosition:1,frontMatter:{title:"COO-LLM Overview",sidebar_position:1,tags:["user-guide","getting-started"]},sidebar:"tutorialSidebar",next:{title:"Configuration",permalink:"/docs/docs/Guides/Configuration"}},s={},p=[{value:"Key Features",id:"key-features",level:2},{value:"\ud83d\ude80 Core Capabilities",id:"-core-capabilities",level:3},{value:"\ud83d\udcb0 Cost &amp; Performance Optimization",id:"-cost--performance-optimization",level:3},{value:"\ud83d\udd27 Enterprise-Ready",id:"-enterprise-ready",level:3},{value:"\ud83d\udcca Advanced Features",id:"-advanced-features",level:3},{value:"Use Cases",id:"use-cases",level:2},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Quick Example",id:"quick-example",level:2},{value:"Getting Started",id:"getting-started",level:2}],u={toc:p},c="wrapper";function g({components:e,...t}){return(0,i.yg)(c,(0,n.A)({},u,t,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("p",null,"COO-LLM is an intelligent reverse proxy and load balancer for Large Language Model (LLM) APIs. It provides a unified, OpenAI-compatible interface to multiple LLM providers while intelligently distributing requests across API keys and providers based on performance, cost, and rate limits."),(0,i.yg)("h2",{id:"key-features"},"Key Features"),(0,i.yg)("h3",{id:"-core-capabilities"},"\ud83d\ude80 Core Capabilities"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"OpenAI API Compatibility"),": Drop-in replacement for OpenAI API with identical request/response formats"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Multi-Provider Support"),": Seamlessly route requests to OpenAI, Google Gemini, Anthropic Claude, and custom providers"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Intelligent Load Balancing"),": Advanced algorithms for optimal request distribution")),(0,i.yg)("h3",{id:"-cost--performance-optimization"},"\ud83d\udcb0 Cost & Performance Optimization"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Real-time Cost Tracking"),": Monitor and optimize API costs across providers"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Rate Limit Management"),": Automatic key rotation to avoid 429 errors"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Performance Monitoring"),": Track latency, success rates, and token usage")),(0,i.yg)("h3",{id:"-enterprise-ready"},"\ud83d\udd27 Enterprise-Ready"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Extensible Architecture"),": Plugin system for custom providers, storage, and logging"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Production Observability"),": Prometheus metrics, structured logging, and health checks"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Configuration Management"),": YAML-based configuration with hot-reload capabilities")),(0,i.yg)("h3",{id:"-advanced-features"},"\ud83d\udcca Advanced Features"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Model Aliases"),": Map custom model names to provider-specific models"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Request Routing"),": Smart routing based on model availability and performance"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Admin API"),": Runtime configuration and monitoring endpoints")),(0,i.yg)("h2",{id:"use-cases"},"Use Cases"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Cost Optimization"),": Automatically choose the cheapest provider for each request"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"High Availability"),": Failover between providers and keys during outages"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Rate Limit Scaling"),": Distribute load across multiple API keys"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Multi-Cloud LLM"),": Unified interface to multiple cloud LLM services"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Development"),": Easy switching between providers during development")),(0,i.yg)("h2",{id:"architecture-overview"},"Architecture Overview"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre"},"Client Apps (OpenAI SDK)\n    \u2193\nCOO-LLM Proxy\n\u251c\u2500\u2500 API Layer (OpenAI-compatible)\n\u251c\u2500\u2500 Load Balancer (Smart routing)\n\u251c\u2500\u2500 Provider Adapters (OpenAI, Gemini, Claude)\n\u251c\u2500\u2500 Storage (Redis/File/HTTP)\n\u2514\u2500\u2500 Logging (File/Prometheus/Webhook)\n    \u2193\nExternal LLM Providers\n")),(0,i.yg)("h2",{id:"quick-example"},"Quick Example"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},'# Configure providers\ncat > config.yaml << EOF\nversion: "1.0"\nserver:\n  listen: ":8080"\n\nllm_providers:\n  - id: "openai-prod"\n    type: "openai"\n    api_keys: ["sk-your-key"]\n    base_url: "https://api.openai.com"\n    model: "gpt-4o"\n    pricing:\n      input_token_cost: 0.002\n      output_token_cost: 0.01\n    limits:\n      req_per_min: 200\n      tokens_per_min: 100000\n\nmodel_aliases:\n  gpt-4o: openai-prod:gpt-4o\nEOF\n\n# Run COO-LLM\n./coo-llm -config config.yaml\n\n# Use like OpenAI API\ncurl -X POST http://localhost:8080/v1/chat/completions \\\n  -H "Authorization: Bearer your-key" \\\n  -d \'{"model": "gpt-4o", "messages": [{"role": "user", "content": "Hello"}]}\'\n')),(0,i.yg)("h2",{id:"getting-started"},"Getting Started"),(0,i.yg)("p",null,"See ",(0,i.yg)("a",{parentName:"p",href:"/docs/docs/Guides/Deployment"},"Deployment")," for installation instructions and ",(0,i.yg)("a",{parentName:"p",href:"/docs/docs/Guides/Configuration"},"Configuration")," for setup details."))}g.isMDXComponent=!0}}]);