"use strict";(globalThis.webpackChunkcoo_llm_docs=globalThis.webpackChunkcoo_llm_docs||[]).push([[694],{8514:e=>{e.exports=JSON.parse('{"label":"developer-guide","permalink":"/coo-llm-main/docs/tags/developer-guide","allTagsPath":"/coo-llm-main/docs/tags","count":7,"items":[{"id":"Reference/API","title":"API Reference","description":"COO-LLM provides OpenAI-compatible REST APIs for LLM interactions, plus administrative endpoints for management.","permalink":"/coo-llm-main/docs/Reference/API"},{"id":"Intro/Architecture","title":"Architecture","description":"This document describes the high-level architecture of COO-LLM, including component interactions and design decisions.","permalink":"/coo-llm-main/docs/Intro/Architecture"},{"id":"Contributing/Changelog","title":"Changelog","description":"All notable changes to COO-LLM will be documented in this file.","permalink":"/coo-llm-main/docs/Contributing/Changelog"},{"id":"Contributing/Guidelines","title":"Contributing","description":"Thank you for your interest in contributing to COO-LLM! This document provides guidelines and information for contributors.","permalink":"/coo-llm-main/docs/Contributing/Guidelines"},{"id":"Reference/Balancer","title":"Load Balancer","description":"The load balancer is the core intelligence of COO-LLM, responsible for selecting optimal provider and API key combinations based on performance, cost, and availability.","permalink":"/coo-llm-main/docs/Reference/Balancer"},{"id":"Reference/Logging","title":"Logging","description":"COO-LLM provides structured logging with file output and Prometheus metrics integration.","permalink":"/coo-llm-main/docs/Reference/Logging"},{"id":"Reference/Storage","title":"Storage","description":"COO-LLM uses pluggable storage backends for runtime metrics and caching. The system supports Redis, in-memory, HTTP API, and file-based storage.","permalink":"/coo-llm-main/docs/Reference/Storage"}]}')}}]);