"use strict";(globalThis.webpackChunkcoo_llm_docs=globalThis.webpackChunkcoo_llm_docs||[]).push([[121],{5680:(e,n,a)=>{a.d(n,{xA:()=>p,yg:()=>u});var l=a(6540);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function o(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);n&&(l=l.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,l)}return a}function t(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?o(Object(a),!0).forEach(function(n){r(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function i(e,n){if(null==e)return{};var a,l,r=function(e,n){if(null==e)return{};var a,l,r={},o=Object.keys(e);for(l=0;l<o.length;l++)a=o[l],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(l=0;l<o.length;l++)a=o[l],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=l.createContext({}),g=function(e){var n=l.useContext(s),a=n;return e&&(a="function"==typeof e?e(n):t(t({},n),e)),a},p=function(e){var n=g(e.components);return l.createElement(s.Provider,{value:n},e.children)},m="mdxType",c={inlineCode:"code",wrapper:function(e){var n=e.children;return l.createElement(l.Fragment,{},n)}},y=l.forwardRef(function(e,n){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),m=g(a),y=r,u=m["".concat(s,".").concat(y)]||m[y]||c[y]||o;return a?l.createElement(u,t(t({ref:n},p),{},{components:a})):l.createElement(u,t({ref:n},p))});function u(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=a.length,t=new Array(o);t[0]=y;var i={};for(var s in n)hasOwnProperty.call(n,s)&&(i[s]=n[s]);i.originalType=e,i[m]="string"==typeof e?e:r,t[1]=i;for(var g=2;g<o;g++)t[g]=a[g];return l.createElement.apply(null,t)}return l.createElement.apply(null,a)}y.displayName="MDXCreateElement"},7795:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>s,contentTitle:()=>t,default:()=>c,frontMatter:()=>o,metadata:()=>i,toc:()=>g});var l=a(8168),r=(a(6540),a(5680));const o={sidebar_position:3,tags:["user-guide","deployment"]},t="Deployment",i={unversionedId:"Guides/Deployment",id:"Guides/Deployment",title:"Deployment",description:"This guide covers deploying COO-LLM in various environments, from development to production.",source:"@site/docs/Guides/Deployment.md",sourceDirName:"Guides",slug:"/Guides/Deployment",permalink:"/docs/docs/Guides/Deployment",draft:!1,editUrl:"https://github.com/your-org/coo-llm/tree/main/docs/docs/docs/Guides/Deployment.md",tags:[{label:"user-guide",permalink:"/docs/docs/tags/user-guide"},{label:"deployment",permalink:"/docs/docs/tags/deployment"}],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,tags:["user-guide","deployment"]},sidebar:"tutorialSidebar",previous:{title:"Configuration",permalink:"/docs/docs/Guides/Configuration"},next:{title:"Providers",permalink:"/docs/docs/Guides/Providers"}},s={},g=[{value:"Quick Start",id:"quick-start",level:2},{value:"Local Development",id:"local-development",level:3},{value:"Docker Deployment",id:"docker-deployment",level:3},{value:"Docker Compose",id:"docker-compose",level:3},{value:"Production Deployment",id:"production-deployment",level:2},{value:"Environment Variables",id:"environment-variables",level:3},{value:"Kubernetes",id:"kubernetes",level:3},{value:"AWS ECS",id:"aws-ecs",level:3},{value:"Docker Swarm",id:"docker-swarm",level:3},{value:"Configuration Management",id:"configuration-management",level:2},{value:"Environment Variables",id:"environment-variables-1",level:3},{value:"Config Files",id:"config-files",level:3},{value:"Networking",id:"networking",level:2},{value:"Load Balancing",id:"load-balancing",level:3},{value:"Security",id:"security",level:3},{value:"Monitoring",id:"monitoring",level:2},{value:"Metrics",id:"metrics",level:3},{value:"Logging",id:"logging",level:3},{value:"Logging",id:"logging-1",level:3},{value:"Scaling",id:"scaling",level:2},{value:"Multiple API Keys",id:"multiple-api-keys",level:3},{value:"Redis for Production",id:"redis-for-production",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Logs",id:"logs",level:3},{value:"Security",id:"security-1",level:2},{value:"API Key Management",id:"api-key-management",level:3},{value:"Best Practices",id:"best-practices",level:3}],p={toc:g},m="wrapper";function c({components:e,...n}){return(0,r.yg)(m,(0,l.A)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"deployment"},"Deployment"),(0,r.yg)("p",null,"This guide covers deploying COO-LLM in various environments, from development to production."),(0,r.yg)("h2",{id:"quick-start"},"Quick Start"),(0,r.yg)("h3",{id:"local-development"},"Local Development"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Clone and build:")),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/your-org/coo-llm.git\ncd coo-llm\ngo build -o coo-llm ./cmd/coo-llm\n"))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Configure environment:")),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'export OPENAI_API_KEY="sk-your-key"\nexport GEMINI_API_KEY="your-gemini-key"\n'))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Create config:")),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'# configs/config.yaml\nversion: "1.0"\nserver:\n  listen: ":8080"\n\nllm_providers:\n  - id: "openai-prod"\n    type: "openai"\n    api_keys: ["${OPENAI_API_KEY}"]\n    base_url: "https://api.openai.com"\n    model: "gpt-4o"\n    pricing:\n      input_token_cost: 0.002\n      output_token_cost: 0.01\n    limits:\n      req_per_min: 200\n      tokens_per_min: 100000\n\napi_keys:\n  - key: "test-key"\n    allowed_providers: ["*"]\n    description: "Test key for development"\n\nmodel_aliases:\n  gpt-4o: openai-prod:gpt-4o\n'))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Run:")),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"./coo-llm -config configs/config.yaml\n"))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Test:")),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'curl -X POST http://localhost:8080/v1/chat/completions \\\n  -H "Authorization: Bearer test-key" \\\n  -d \'{"model": "gpt-4o", "messages": [{"role": "user", "content": "Hello"}]}\'\n')))),(0,r.yg)("h3",{id:"docker-deployment"},"Docker Deployment"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Build image:")),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"docker build -t coo-llm:latest .\n"))),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Run container:")),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'docker run -p 8080:8080 \\\n  -e OPENAI_API_KEY="sk-your-key" \\\n  -e GEMINI_API_KEY="your-gemini-key" \\\n  -v $(pwd)/configs:/app/configs \\\n  -v $(pwd)/logs:/app/logs \\\n  coo-llm:latest\n')))),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Dockerfile details:")),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Multi-stage build with Go 1.23"),(0,r.yg)("li",{parentName:"ul"},"Alpine Linux base image for minimal size"),(0,r.yg)("li",{parentName:"ul"},"Non-root user for security"),(0,r.yg)("li",{parentName:"ul"},"Default config included in image")),(0,r.yg)("h3",{id:"docker-compose"},"Docker Compose"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Full stack with Redis:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'# docker-compose.yml\nversion: \'3.8\'\nservices:\n  coo-llm:\n    build: .\n    ports:\n      - "8080:8080"\n    environment:\n      - CONFIG_PATH=/app/configs/config.yaml\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - GEMINI_API_KEY=${GEMINI_API_KEY}\n    depends_on:\n      - redis\n    volumes:\n      - ./logs:/app/logs\n\n  redis:\n    image: redis:7\n    ports:\n      - "6379:6379"\n    volumes:\n      - redis_data:/data\n\nvolumes:\n  redis_data:\n')),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Redis-only for development:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"# docker-compose.local.yml\nversion: '3.8'\nservices:\n  redis:\n    image: redis:7\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_data:/data\n\nvolumes:\n  redis_data:\n")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Run:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"# Full stack\ndocker-compose up -d\n\n# Redis only (for local development)\ndocker-compose -f docker-compose.local.yml up -d\n")),(0,r.yg)("h2",{id:"production-deployment"},"Production Deployment"),(0,r.yg)("h3",{id:"environment-variables"},"Environment Variables"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Required:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'export OPENAI_API_KEY="sk-your-key"\nexport GEMINI_API_KEY="your-gemini-key"  # Optional\nexport CLAUDE_API_KEY="your-claude-key"  # Optional\n')),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Optional:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'export REDIS_ADDR="redis:6379"           # For Redis storage\nexport REDIS_PASSWORD="your-password"    # If Redis requires auth\nexport ADMIN_API_KEY="your-admin-key"    # For admin endpoints (future)\n')),(0,r.yg)("h3",{id:"kubernetes"},"Kubernetes"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Basic Deployment:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: coo-llm\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: coo-llm\n  template:\n    metadata:\n      labels:\n        app: coo-llm\n    spec:\n      containers:\n      - name: coo-llm\n        image: coo-llm:latest\n        ports:\n        - containerPort: 8080\n        env:\n        - name: OPENAI_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: llm-secrets\n              key: openai-key\n        - name: REDIS_ADDR\n          value: "redis-service:6379"\n        volumeMounts:\n        - name: config\n          mountPath: /app/configs\n      volumes:\n      - name: config\n        configMap:\n          name: coo-llm-config\n')),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Service:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: Service\nmetadata:\n  name: coo-llm\nspec:\n  selector:\n    app: coo-llm\n  ports:\n  - port: 80\n    targetPort: 8080\n  type: ClusterIP\n")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"ConfigMap:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coo-llm-config\ndata:\n  config.yaml: |\n    version: "1.0"\n    server:\n      listen: ":8080"\n    storage:\n      runtime:\n        type: redis\n        addr: "${REDIS_ADDR}"\n    llm_providers:\n      - id: openai\n        type: openai\n        api_keys: ["${OPENAI_API_KEY}"]\n        model: gpt-4o\n    model_aliases:\n      gpt-4o: openai:gpt-4o\n')),(0,r.yg)("h3",{id:"aws-ecs"},"AWS ECS"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Task Definition:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "family": "coo-llm",\n  "taskRoleArn": "arn:aws:iam::123456789012:role/ecsTaskRole",\n  "executionRoleArn": "arn:aws:iam::123456789012:role/ecsTaskExecutionRole",\n  "networkMode": "awsvpc",\n  "requiresCompatibilities": ["FARGATE"],\n  "cpu": "256",\n  "memory": "512",\n  "containerDefinitions": [\n    {\n      "name": "coo-llm",\n      "image": "coo-llm:latest",\n      "essential": true,\n      "portMappings": [\n        {\n          "containerPort": 8080,\n          "hostPort": 8080\n        }\n      ],\n      "environment": [\n        {\n          "name": "OPENAI_API_KEY",\n          "valueFrom": "arn:aws:secretsmanager:region:123456789012:secret:openai-key"\n        }\n      ],\n      "logConfiguration": {\n        "logDriver": "awslogs",\n        "options": {\n          "awslogs-group": "/ecs/coo-llm",\n          "awslogs-region": "us-east-1",\n          "awslogs-stream-prefix": "ecs"\n        }\n      }\n    }\n  ]\n}\n')),(0,r.yg)("h3",{id:"docker-swarm"},"Docker Swarm"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"# docker-compose.swarm.yml\nversion: '3.8'\nservices:\n  coo-llm:\n    image: coo-llm:latest\n    ports:\n      - \"8080:8080\"\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n    configs:\n      - source: truckllm_config\n        target: /app/configs/config.yaml\n    deploy:\n      mode: replicated\n      replicas: 3\n      restart_policy:\n        condition: on-failure\n\n  redis:\n    image: redis:7-alpine\n    volumes:\n      - redis_data:/data\n    deploy:\n      mode: global\n\nconfigs:\n  truckllm_config:\n    file: ./configs/config.yaml\n\nvolumes:\n  redis_data:\n")),(0,r.yg)("h2",{id:"configuration-management"},"Configuration Management"),(0,r.yg)("h3",{id:"environment-variables-1"},"Environment Variables"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Development:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'export OPENAI_API_KEY="sk-..."\nexport GEMINI_API_KEY="..."\nexport CLAUDE_API_KEY="..."\nexport REDIS_URL="redis://localhost:6379"\n')),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Production:"),"\nUse secret management services:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"AWS Secrets Manager"),(0,r.yg)("li",{parentName:"ul"},"Google Secret Manager"),(0,r.yg)("li",{parentName:"ul"},"Azure Key Vault"),(0,r.yg)("li",{parentName:"ul"},"HashiCorp Vault")),(0,r.yg)("h3",{id:"config-files"},"Config Files"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Directory Structure:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"configs/\n\u251c\u2500\u2500 config.yaml          # Main config\n\u251c\u2500\u2500 config.prod.yaml     # Production overrides\n\u251c\u2500\u2500 config.dev.yaml      # Development overrides\n\u2514\u2500\u2500 secrets/             # Encrypted secrets\n")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Environment-specific configs:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"# Development\n./coo-llm -config configs/config.dev.yaml\n\n# Production\n./coo-llm -config configs/config.prod.yaml\n")),(0,r.yg)("h2",{id:"networking"},"Networking"),(0,r.yg)("h3",{id:"load-balancing"},"Load Balancing"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"nginx:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-nginx"},"upstream coo-llm {\n    server coo-llm-1:8080;\n    server coo-llm-2:8080;\n    server coo-llm-3:8080;\n}\n\nserver {\n    listen 80;\n    server_name api.example.com;\n\n    location / {\n        proxy_pass http://coo-llm;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n    }\n}\n")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"AWS ALB:")),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Target Group: EC2 instances or ECS tasks"),(0,r.yg)("li",{parentName:"ul"},"Health Check: ",(0,r.yg)("inlineCode",{parentName:"li"},"GET /health")),(0,r.yg)("li",{parentName:"ul"},"SSL Termination: ACM certificate")),(0,r.yg)("h3",{id:"security"},"Security"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"TLS Configuration:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'server:\n  listen: ":8443"\n  tls:\n    cert_file: "/etc/ssl/certs/coo-llm.crt"\n    key_file: "/etc/ssl/private/coo-llm.key"\n')),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"nginx with TLS:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-nginx"},"server {\n    listen 443 ssl;\n    server_name api.example.com;\n\n    ssl_certificate /etc/ssl/certs/api.crt;\n    ssl_certificate_key /etc/ssl/private/api.key;\n\n    location / {\n        proxy_pass http://coo-llm;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n")),(0,r.yg)("h2",{id:"monitoring"},"Monitoring"),(0,r.yg)("h3",{id:"metrics"},"Metrics"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Prometheus metrics are available at ",(0,r.yg)("inlineCode",{parentName:"strong"},"/metrics")," when enabled:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'logging:\n  prometheus:\n    enabled: true\n    endpoint: "/metrics"\n')),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Prometheus config:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"scrape_configs:\n  - job_name: 'coo-llm'\n    static_configs:\n      - targets: ['coo-llm:8080']\n    metrics_path: '/metrics'\n")),(0,r.yg)("h3",{id:"logging"},"Logging"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"File logging:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'logging:\n  file:\n    enabled: true\n    path: "./logs/llm.log"\n    max_size_mb: 100\n    max_backups: 5\n')),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Logs include:")),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Request details (model, tokens, latency)"),(0,r.yg)("li",{parentName:"ul"},"Provider selection"),(0,r.yg)("li",{parentName:"ul"},"Errors and retries"),(0,r.yg)("li",{parentName:"ul"},"Usage metrics")),(0,r.yg)("h3",{id:"logging-1"},"Logging"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Centralized Logging:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'logging:\n  providers:\n    - name: "elasticsearch"\n      type: "http"\n      endpoint: "https://es.example.com/_bulk"\n')),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Log Aggregation:")),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"ELK Stack (Elasticsearch, Logstash, Kibana)"),(0,r.yg)("li",{parentName:"ul"},"Splunk"),(0,r.yg)("li",{parentName:"ul"},"Datadog"),(0,r.yg)("li",{parentName:"ul"},"CloudWatch Logs")),(0,r.yg)("h2",{id:"scaling"},"Scaling"),(0,r.yg)("h3",{id:"multiple-api-keys"},"Multiple API Keys"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Add multiple keys per provider for higher throughput:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'llm_providers:\n  - id: "openai-prod"\n    type: "openai"\n    api_keys: ["${OPENAI_KEY_1}", "${OPENAI_KEY_2}", "${OPENAI_KEY_3}"]\n    limits:\n      req_per_min: 200  # Per key\n      tokens_per_min: 100000\n')),(0,r.yg)("h3",{id:"redis-for-production"},"Redis for Production"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Use Redis for persistent metrics and caching:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'storage:\n  runtime:\n    type: "redis"\n    addr: "redis:6379"\n    password: "${REDIS_PASSWORD}"\n')),(0,r.yg)("h2",{id:"troubleshooting"},"Troubleshooting"),(0,r.yg)("h3",{id:"common-issues"},"Common Issues"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Application won't start:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"# Check config syntax\n./coo-llm -config configs/config.yaml\n\n# Check environment variables\necho $OPENAI_API_KEY\n")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Requests failing:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'# Test basic connectivity\ncurl -X POST http://localhost:8080/v1/chat/completions \\\n  -H "Authorization: Bearer test-key" \\\n  -d \'{"model": "gpt-4o", "messages": [{"role": "user", "content": "Hello"}]}\'\n\n# Check logs\ntail -f logs/llm.log\n')),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Rate limiting errors:")),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Check your API key limits with the provider"),(0,r.yg)("li",{parentName:"ul"},"Add more API keys to your configuration"),(0,r.yg)("li",{parentName:"ul"},"Reduce request frequency")),(0,r.yg)("h3",{id:"logs"},"Logs"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Log files are written to ",(0,r.yg)("inlineCode",{parentName:"strong"},"./logs/llm.log")," by default:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'logging:\n  file:\n    enabled: true\n    path: "./logs/llm.log"\n    max_size_mb: 100\n    max_backups: 5\n')),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Log entries include:")),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Request timestamps and models"),(0,r.yg)("li",{parentName:"ul"},"Token usage and latency"),(0,r.yg)("li",{parentName:"ul"},"Provider selection decisions"),(0,r.yg)("li",{parentName:"ul"},"Errors and retry attempts")),(0,r.yg)("h2",{id:"security-1"},"Security"),(0,r.yg)("h3",{id:"api-key-management"},"API Key Management"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Store API keys securely:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'# Environment variables (recommended)\nexport OPENAI_API_KEY="sk-your-secure-key"\n\n# Or use a secrets file (not in repo)\necho "OPENAI_API_KEY=sk-your-key" > .env\n')),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Client authentication:")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'api_keys:\n  - key: "production-key"\n    allowed_providers: ["openai-prod", "gemini-prod"]\n    description: "Production client"\n')),(0,r.yg)("h3",{id:"best-practices"},"Best Practices"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Use different API keys for different environments"),(0,r.yg)("li",{parentName:"ul"},"Rotate keys regularly"),(0,r.yg)("li",{parentName:"ul"},"Monitor usage patterns"),(0,r.yg)("li",{parentName:"ul"},"Use HTTPS in production"),(0,r.yg)("li",{parentName:"ul"},"Limit API key permissions with ",(0,r.yg)("inlineCode",{parentName:"li"},"allowed_providers"))))}c.isMDXComponent=!0}}]);