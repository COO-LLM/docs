"use strict";(globalThis.webpackChunkcoo_llm_docs=globalThis.webpackChunkcoo_llm_docs||[]).push([[309],{5680:(e,n,r)=>{r.d(n,{xA:()=>u,yg:()=>c});var t=r(6540);function i(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function a(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),r.push.apply(r,t)}return r}function o(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?a(Object(r),!0).forEach(function(n){i(e,n,r[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):a(Object(r)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))})}return e}function l(e,n){if(null==e)return{};var r,t,i=function(e,n){if(null==e)return{};var r,t,i={},a=Object.keys(e);for(t=0;t<a.length;t++)r=a[t],n.indexOf(r)>=0||(i[r]=e[r]);return i}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(t=0;t<a.length;t++)r=a[t],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(i[r]=e[r])}return i}var s=t.createContext({}),p=function(e){var n=t.useContext(s),r=n;return e&&(r="function"==typeof e?e(n):o(o({},n),e)),r},u=function(e){var n=p(e.components);return t.createElement(s.Provider,{value:n},e.children)},g="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},d=t.forwardRef(function(e,n){var r=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),g=p(r),d=i,c=g["".concat(s,".").concat(d)]||g[d]||m[d]||a;return r?t.createElement(c,o(o({ref:n},u),{},{components:r})):t.createElement(c,o({ref:n},u))});function c(e,n){var r=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var a=r.length,o=new Array(a);o[0]=d;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[g]="string"==typeof e?e:i,o[1]=l;for(var p=2;p<a;p++)o[p]=r[p];return t.createElement.apply(null,o)}return t.createElement.apply(null,r)}d.displayName="MDXCreateElement"},8960:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>m,frontMatter:()=>a,metadata:()=>l,toc:()=>p});var t=r(8168),i=(r(6540),r(5680));const a={sidebar_position:4,tags:["user-guide","providers"]},o="Providers",l={unversionedId:"Guides/Providers",id:"Guides/Providers",title:"Providers",description:"COO-LLM supports multiple LLM providers through a plugin-based architecture. Each provider implements a common interface for seamless integration.",source:"@site/docs/Guides/Providers.md",sourceDirName:"Guides",slug:"/Guides/Providers",permalink:"/coo-llm-main/docs/Guides/Providers",draft:!1,editUrl:"https://github.com/your-org/coo-llm/tree/main/docs/docs/docs/Guides/Providers.md",tags:[{label:"user-guide",permalink:"/coo-llm-main/docs/tags/user-guide"},{label:"providers",permalink:"/coo-llm-main/docs/tags/providers"}],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4,tags:["user-guide","providers"]},sidebar:"tutorialSidebar",previous:{title:"Deployment",permalink:"/coo-llm-main/docs/Guides/Deployment"},next:{title:"Architecture",permalink:"/coo-llm-main/docs/Intro/Architecture"}},s={},p=[{value:"Supported Providers",id:"supported-providers",level:2},{value:"OpenAI",id:"openai",level:3},{value:"Google Gemini",id:"google-gemini",level:3},{value:"Anthropic Claude",id:"anthropic-claude",level:3},{value:"Custom Provider",id:"custom-provider",level:3},{value:"Provider Interface",id:"provider-interface",level:2},{value:"Request Structure",id:"request-structure",level:3},{value:"Response Structure",id:"response-structure",level:3},{value:"Configuration Structure",id:"configuration-structure",level:3},{value:"Adding Custom Providers",id:"adding-custom-providers",level:2},{value:"Provider-Specific Features",id:"provider-specific-features",level:2},{value:"OpenAI Features",id:"openai-features",level:3},{value:"Gemini Features",id:"gemini-features",level:3},{value:"Claude Features",id:"claude-features",level:3},{value:"Pricing Integration",id:"pricing-integration",level:2},{value:"Rate Limiting",id:"rate-limiting",level:2},{value:"Error Handling",id:"error-handling",level:2},{value:"Monitoring",id:"monitoring",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Key Management",id:"key-management",level:3},{value:"Cost Optimization",id:"cost-optimization",level:3},{value:"Reliability",id:"reliability",level:3},{value:"Security",id:"security",level:3}],u={toc:p},g="wrapper";function m({components:e,...n}){return(0,i.yg)(g,(0,t.A)({},u,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"providers"},"Providers"),(0,i.yg)("p",null,"COO-LLM supports multiple LLM providers through a plugin-based architecture. Each provider implements a common interface for seamless integration."),(0,i.yg)("h2",{id:"supported-providers"},"Supported Providers"),(0,i.yg)("h3",{id:"openai"},"OpenAI"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Provider Type:")," ",(0,i.yg)("inlineCode",{parentName:"p"},"openai")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Configuration:")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},'llm_providers:\n  - id: "openai-prod"\n    type: "openai"\n    api_keys: ["${OPENAI_KEY_1}", "${OPENAI_KEY_2}"]\n    base_url: "https://api.openai.com"\n    model: "gpt-4o"\n    pricing:\n      input_token_cost: 0.002\n      output_token_cost: 0.01\n    limits:\n      req_per_min: 200\n      tokens_per_min: 100000\n')),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Supported Models:")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"gpt-4")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"gpt-4-turbo")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"gpt-4o")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"gpt-3.5-turbo")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"gpt-3.5-turbo-instruct"))),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Features:")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Chat completions with conversation history"),(0,i.yg)("li",{parentName:"ul"},"Token usage tracking"),(0,i.yg)("li",{parentName:"ul"},"Error handling and retries"),(0,i.yg)("li",{parentName:"ul"},"Rate limit management")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Rate Limits:")," Based on OpenAI tier (see ",(0,i.yg)("a",{parentName:"p",href:"https://platform.openai.com/docs/guides/rate-limits"},"OpenAI docs"),")"),(0,i.yg)("h3",{id:"google-gemini"},"Google Gemini"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Provider Type:")," ",(0,i.yg)("inlineCode",{parentName:"p"},"gemini")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Configuration:")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},'llm_providers:\n  - id: "gemini-prod"\n    type: "gemini"\n    api_keys: ["${GEMINI_KEY_1}"]\n    base_url: "https://generativelanguage.googleapis.com"\n    model: "gemini-1.5-pro"\n    pricing:\n      input_token_cost: 0.00025\n      output_token_cost: 0.0005\n    limits:\n      req_per_min: 150\n      tokens_per_min: 80000\n')),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Supported Models:")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"gemini-1.5-pro")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"gemini-1.5-flash")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"gemini-pro"))),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Features:")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Chat completions"),(0,i.yg)("li",{parentName:"ul"},"Token usage tracking"),(0,i.yg)("li",{parentName:"ul"},"Error handling")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Rate Limits:")," Varies by tier (see ",(0,i.yg)("a",{parentName:"p",href:"https://ai.google.dev/docs/rate-limits"},"Gemini docs"),")"),(0,i.yg)("h3",{id:"anthropic-claude"},"Anthropic Claude"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Provider Type:")," ",(0,i.yg)("inlineCode",{parentName:"p"},"claude")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Configuration:")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},'llm_providers:\n  - id: "claude-prod"\n    type: "claude"\n    api_keys: ["${CLAUDE_KEY_1}"]\n    base_url: "https://api.anthropic.com"\n    model: "claude-3-opus"\n    pricing:\n      input_token_cost: 0.015\n      output_token_cost: 0.075\n    limits:\n      req_per_min: 100\n      tokens_per_min: 60000\n')),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Supported Models:")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"claude-3-opus")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"claude-3-sonnet")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"claude-3-haiku")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"claude-2.1"))),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Features:")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Chat completions"),(0,i.yg)("li",{parentName:"ul"},"Token usage tracking"),(0,i.yg)("li",{parentName:"ul"},"Error handling")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Rate Limits:")," Varies by model (see ",(0,i.yg)("a",{parentName:"p",href:"https://docs.anthropic.com/claude/docs/rate-limits"},"Anthropic docs"),")"),(0,i.yg)("h3",{id:"custom-provider"},"Custom Provider"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Provider Type:")," ",(0,i.yg)("inlineCode",{parentName:"p"},"custom")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Configuration:")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},'llm_providers:\n  - id: "custom-dev"\n    type: "custom"\n    api_keys: ["${CUSTOM_KEY_1}"]\n    base_url: "https://custom-llm.example.com"\n    model: "custom-model"\n    pricing:\n      input_token_cost: 0.001\n      output_token_cost: 0.002\n    limits:\n      req_per_min: 50\n      tokens_per_min: 10000\n')),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Use Case:")," Integrate with proprietary or custom LLM APIs"),(0,i.yg)("h2",{id:"provider-interface"},"Provider Interface"),(0,i.yg)("p",null,"All providers implement the ",(0,i.yg)("inlineCode",{parentName:"p"},"LLMProvider")," interface defined in ",(0,i.yg)("inlineCode",{parentName:"p"},"internal/provider/interface.go"),":"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-go"},"type LLMProvider interface {\n    Name() string\n    Generate(ctx context.Context, req *LLMRequest) (*LLMResponse, error)\n    ListModels(ctx context.Context) ([]string, error)\n}\n")),(0,i.yg)("h3",{id:"request-structure"},"Request Structure"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-go"},'type LLMRequest struct {\n    Prompt    string                   `json:"prompt"`\n    Messages  []map[string]interface{} `json:"messages,omitempty"`\n    Model     string                   `json:"model,omitempty"`\n    MaxTokens int                      `json:"max_tokens,omitempty"`\n    Params    map[string]any           `json:"params,omitempty"`\n}\n')),(0,i.yg)("h3",{id:"response-structure"},"Response Structure"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-go"},'type LLMResponse struct {\n    Text         string `json:"text"`\n    InputTokens  int    `json:"input_tokens"`\n    OutputTokens int    `json:"output_tokens"`\n    TokensUsed   int    `json:"tokens_used"`\n    FinishReason string `json:"finish_reason"`\n}\n')),(0,i.yg)("h3",{id:"configuration-structure"},"Configuration Structure"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-go"},'type LLMConfig struct {\n    Type    ProviderType `yaml:"type"`\n    APIKeys []string     `yaml:"api_keys"`\n    BaseURL string       `yaml:"base_url,omitempty"`\n    Model   string       `yaml:"model"`\n    Pricing Pricing      `yaml:"pricing"`\n    Limits  Limits       `yaml:"limits"`\n}\n')),(0,i.yg)("h2",{id:"adding-custom-providers"},"Adding Custom Providers"),(0,i.yg)("p",null,"To add a new provider, implement the ",(0,i.yg)("inlineCode",{parentName:"p"},"LLMProvider")," interface and register it in the registry."),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Create a new provider file (e.g., ",(0,i.yg)("inlineCode",{parentName:"strong"},"internal/provider/custom.go"),"):"))),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-go"},'package provider\n\nimport (\n    "bytes"\n    "context"\n    "encoding/json"\n    "fmt"\n    "io"\n    "net/http"\n    "time"\n)\n\ntype CustomProvider struct {\n    config LLMConfig\n}\n\nfunc NewCustomProvider(config LLMConfig) LLMProvider {\n    return &CustomProvider{config: config}\n}\n\nfunc (p *CustomProvider) Name() string {\n    return "custom" // Provider type identifier\n}\n\nfunc (p *CustomProvider) Generate(ctx context.Context, req *LLMRequest) (*LLMResponse, error) {\n    // Get API key (round-robin if multiple)\n    apiKey := p.config.APIKey()\n    if apiKey == "" {\n        return nil, fmt.Errorf("no API key available")\n    }\n\n    // Transform to provider-specific request format\n    providerReq := map[string]interface{}{\n        "model": req.Model,\n        "messages": req.Messages,\n        "max_tokens": req.MaxTokens,\n    }\n\n    // Make HTTP request\n    jsonData, _ := json.Marshal(providerReq)\n    httpReq, err := http.NewRequestWithContext(ctx, "POST",\n        p.config.BaseURL+"/chat/completions", bytes.NewBuffer(jsonData))\n    if err != nil {\n        return nil, err\n    }\n\n    httpReq.Header.Set("Content-Type", "application/json")\n    httpReq.Header.Set("Authorization", "Bearer "+apiKey)\n\n    client := &http.Client{Timeout: 30 * time.Second}\n    resp, err := client.Do(httpReq)\n    if err != nil {\n        return nil, err\n    }\n    defer resp.Body.Close()\n\n    // Read response\n    body, err := io.ReadAll(resp.Body)\n    if err != nil {\n        return nil, err\n    }\n\n    if resp.StatusCode != 200 {\n        return nil, fmt.Errorf("provider error: %s", string(body))\n    }\n\n    // Parse provider response\n    var providerResp map[string]interface{}\n    if err := json.Unmarshal(body, &providerResp); err != nil {\n        return nil, err\n    }\n\n    // Extract response data\n    choices := providerResp["choices"].([]interface{})\n    if len(choices) == 0 {\n        return nil, fmt.Errorf("no choices in response")\n    }\n\n    choice := choices[0].(map[string]interface{})\n    message := choice["message"].(map[string]interface{})\n    text := message["content"].(string)\n\n    // Extract usage (adjust based on provider\'s response format)\n    usage := providerResp["usage"].(map[string]interface{})\n    inputTokens := int(usage["prompt_tokens"].(float64))\n    outputTokens := int(usage["completion_tokens"].(float64))\n\n    return &LLMResponse{\n        Text:         text,\n        InputTokens:  inputTokens,\n        OutputTokens: outputTokens,\n        TokensUsed:   inputTokens + outputTokens,\n        FinishReason: choice["finish_reason"].(string),\n    }, nil\n}\n\nfunc (p *CustomProvider) ListModels(ctx context.Context) ([]string, error) {\n    // Return supported models for this provider\n    return []string{p.config.Model}, nil\n}\n')),(0,i.yg)("ol",{start:2},(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Register the provider in ",(0,i.yg)("inlineCode",{parentName:"strong"},"internal/provider/registry.go"),":"))),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-go"},'func (r *Registry) LoadFromConfig(cfg *config.Config) error {\n    // ... existing code for LLMProviders ...\n\n    for _, lp := range cfg.LLMProviders {\n        llmCfg := LLMConfig{\n            Type:    ProviderType(lp.Type),\n            APIKeys: lp.APIKeys,\n            BaseURL: lp.BaseURL,\n            Model:   lp.Model,\n            Pricing: lp.Pricing,\n            Limits:  lp.Limits,\n        }\n        var p LLMProvider\n        switch lp.Type {\n        case "openai":\n            p = NewOpenAIProvider(llmCfg)\n        case "gemini":\n            p = NewGeminiProvider(llmCfg)\n        case "claude":\n            p = NewClaudeProvider(llmCfg)\n        case "custom":\n            p = NewCustomProvider(llmCfg)\n        default:\n            return fmt.Errorf("unsupported provider type: %s", lp.Type)\n        }\n        r.Register(p)\n    }\n    return nil\n}\n')),(0,i.yg)("ol",{start:3},(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Add to configuration:"))),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},'llm_providers:\n  - id: "my-custom"\n    type: "custom"\n    api_keys: ["${CUSTOM_API_KEY}"]\n    base_url: "https://api.my-custom-provider.com/v1"\n    model: "my-model"\n    pricing:\n      input_token_cost: 0.001\n      output_token_cost: 0.002\n    limits:\n      req_per_min: 100\n      tokens_per_min: 50000\n\nmodel_aliases:\n  my-model: my-custom:my-model\n')),(0,i.yg)("h2",{id:"provider-specific-features"},"Provider-Specific Features"),(0,i.yg)("h3",{id:"openai-features"},"OpenAI Features"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Streaming responses"),(0,i.yg)("li",{parentName:"ul"},"Function calling"),(0,i.yg)("li",{parentName:"ul"},"Vision models"),(0,i.yg)("li",{parentName:"ul"},"Fine-tuned models")),(0,i.yg)("h3",{id:"gemini-features"},"Gemini Features"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Multimodal inputs (text, images)"),(0,i.yg)("li",{parentName:"ul"},"Function calling"),(0,i.yg)("li",{parentName:"ul"},"Grounding with Google Search"),(0,i.yg)("li",{parentName:"ul"},"Code execution")),(0,i.yg)("h3",{id:"claude-features"},"Claude Features"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Large context windows (200K tokens)"),(0,i.yg)("li",{parentName:"ul"},"Advanced reasoning"),(0,i.yg)("li",{parentName:"ul"},"Constitutional AI safety")),(0,i.yg)("h2",{id:"pricing-integration"},"Pricing Integration"),(0,i.yg)("p",null,"Each provider includes real-time pricing information:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Input Token Cost:")," Cost per 1,000 input tokens"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Output Token Cost:")," Cost per 1,000 output tokens"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Currency:")," Billing currency (typically USD)")),(0,i.yg)("p",null,"Pricing data is used for cost optimization in load balancing."),(0,i.yg)("h2",{id:"rate-limiting"},"Rate Limiting"),(0,i.yg)("p",null,"Each key has configurable rate limits:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Requests per Minute:")," Maximum API calls per minute"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Tokens per Minute:")," Maximum tokens processed per minute")),(0,i.yg)("p",null,"COO-LLM automatically rotates keys when limits are approached."),(0,i.yg)("h2",{id:"error-handling"},"Error Handling"),(0,i.yg)("p",null,"Providers handle various error conditions:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Rate Limits (429):")," Automatic retry with backoff"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Authentication Errors (401/403):")," Key rotation"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Server Errors (5xx):")," Failover to alternative providers"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Model Not Found (404):")," Fallback model selection")),(0,i.yg)("h2",{id:"monitoring"},"Monitoring"),(0,i.yg)("p",null,"Provider performance is tracked:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Request latency"),(0,i.yg)("li",{parentName:"ul"},"Success/error rates"),(0,i.yg)("li",{parentName:"ul"},"Token usage"),(0,i.yg)("li",{parentName:"ul"},"Cost accumulation")),(0,i.yg)("p",null,"Metrics are exposed via Prometheus and admin APIs."),(0,i.yg)("h2",{id:"best-practices"},"Best Practices"),(0,i.yg)("h3",{id:"key-management"},"Key Management"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Use multiple API keys per provider for redundancy"),(0,i.yg)("li",{parentName:"ul"},"Rotate keys regularly for security"),(0,i.yg)("li",{parentName:"ul"},"Monitor key usage and limits")),(0,i.yg)("h3",{id:"cost-optimization"},"Cost Optimization"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Configure accurate pricing information"),(0,i.yg)("li",{parentName:"ul"},"Use cost-first strategies for budget-conscious deployments"),(0,i.yg)("li",{parentName:"ul"},"Monitor spending via admin APIs")),(0,i.yg)("h3",{id:"reliability"},"Reliability"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Configure multiple providers for failover"),(0,i.yg)("li",{parentName:"ul"},"Set appropriate rate limits"),(0,i.yg)("li",{parentName:"ul"},"Monitor error rates and latency")),(0,i.yg)("h3",{id:"security"},"Security"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Store API keys securely (environment variables)"),(0,i.yg)("li",{parentName:"ul"},"Use HTTPS for all provider communications"),(0,i.yg)("li",{parentName:"ul"},"Implement proper authentication and authorization")))}m.isMDXComponent=!0}}]);