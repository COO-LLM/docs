"use strict";(globalThis.webpackChunkcoo_llm_docs=globalThis.webpackChunkcoo_llm_docs||[]).push([[5081],{8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var t=i(6540);const r={},s=t.createContext(r);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(s.Provider,{value:n},e.children)}},8739:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"Intro/Overview","title":"COO-LLM Overview","description":"COO-LLM is an intelligent reverse proxy and load balancer for Large Language Model (LLM) APIs. It provides a unified, OpenAI-compatible interface to multiple LLM providers while intelligently distributing requests across API keys and providers based on performance, cost, and rate limits.","source":"@site/content/Intro/Overview.md","sourceDirName":"Intro","slug":"/Intro/Overview","permalink":"/docs/Intro/Overview","draft":false,"unlisted":false,"editUrl":"https://github.com/coo-llm/coo-llm-main/tree/main/docs/content/content/Intro/Overview.md","tags":[{"inline":true,"label":"user-guide","permalink":"/docs/tags/user-guide"},{"inline":true,"label":"getting-started","permalink":"/docs/tags/getting-started"}],"version":"current","sidebarPosition":1,"frontMatter":{"title":"COO-LLM Overview","sidebar_position":1,"tags":["user-guide","getting-started"]},"sidebar":"tutorialSidebar","next":{"title":"Configuration","permalink":"/docs/Guides/Configuration"}}');var r=i(4848),s=i(8453);const o={title:"COO-LLM Overview",sidebar_position:1,tags:["user-guide","getting-started"]},l=void 0,a={},d=[{value:"Key Features",id:"key-features",level:2},{value:"\ud83d\ude80 Core Capabilities",id:"-core-capabilities",level:3},{value:"\ud83d\udcb0 Cost &amp; Performance Optimization",id:"-cost--performance-optimization",level:3},{value:"\ud83d\udd27 Enterprise-Ready",id:"-enterprise-ready",level:3},{value:"\ud83d\udcca Advanced Features",id:"-advanced-features",level:3},{value:"Use Cases",id:"use-cases",level:2},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Quick Example",id:"quick-example",level:2},{value:"Getting Started",id:"getting-started",level:2}];function c(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"COO-LLM is an intelligent reverse proxy and load balancer for Large Language Model (LLM) APIs. It provides a unified, OpenAI-compatible interface to multiple LLM providers while intelligently distributing requests across API keys and providers based on performance, cost, and rate limits."}),"\n",(0,r.jsx)(n.h2,{id:"key-features",children:"Key Features"}),"\n",(0,r.jsx)(n.h3,{id:"-core-capabilities",children:"\ud83d\ude80 Core Capabilities"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenAI API Compatibility"}),": Drop-in replacement for OpenAI API with identical request/response formats"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-Provider Support"}),": Seamlessly route requests to OpenAI, Google Gemini, Anthropic Claude, and custom providers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Intelligent Load Balancing"}),": Advanced algorithms for optimal request distribution"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-cost--performance-optimization",children:"\ud83d\udcb0 Cost & Performance Optimization"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Cost Tracking"}),": Monitor and optimize API costs across providers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rate Limit Management"}),": Automatic key rotation to avoid 429 errors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance Monitoring"}),": Track latency, success rates, and token usage"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-enterprise-ready",children:"\ud83d\udd27 Enterprise-Ready"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Extensible Architecture"}),": Plugin system for custom providers, storage, and logging"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Production Observability"}),": Prometheus metrics, structured logging, and health checks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Configuration Management"}),": YAML-based configuration with hot-reload capabilities"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-advanced-features",children:"\ud83d\udcca Advanced Features"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Aliases"}),": Map custom model names to provider-specific models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Request Routing"}),": Smart routing based on model availability and performance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Admin API"}),": Runtime configuration and monitoring endpoints"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"use-cases",children:"Use Cases"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost Optimization"}),": Automatically choose the cheapest provider for each request"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"High Availability"}),": Failover between providers and keys during outages"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rate Limit Scaling"}),": Distribute load across multiple API keys"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-Cloud LLM"}),": Unified interface to multiple cloud LLM services"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Development"}),": Easy switching between providers during development"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Client Apps (OpenAI SDK)\n    \u2193\nCOO-LLM Proxy\n\u251c\u2500\u2500 API Layer (OpenAI-compatible)\n\u251c\u2500\u2500 Load Balancer (Smart routing)\n\u251c\u2500\u2500 Provider Adapters (OpenAI, Gemini, Claude)\n\u251c\u2500\u2500 Storage (Redis/File/HTTP)\n\u2514\u2500\u2500 Logging (File/Prometheus/Webhook)\n    \u2193\nExternal LLM Providers\n"})}),"\n",(0,r.jsx)(n.h2,{id:"quick-example",children:"Quick Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Configure providers\ncat > config.yaml << EOF\nversion: "1.0"\nserver:\n  listen: ":8080"\n\nllm_providers:\n  - id: "openai-prod"\n    type: "openai"\n    api_keys: ["sk-your-key"]\n    base_url: "https://api.openai.com"\n    model: "gpt-4o"\n    pricing:\n      input_token_cost: 0.002\n      output_token_cost: 0.01\n    limits:\n      req_per_min: 200\n      tokens_per_min: 100000\n\nmodel_aliases:\n  gpt-4o: openai-prod:gpt-4o\nEOF\n\n# Run COO-LLM\n./coo-llm -config config.yaml\n\n# Use like OpenAI API\ncurl -X POST http://localhost:8080/v1/chat/completions \\\n  -H "Authorization: Bearer your-key" \\\n  -d \'{"model": "gpt-4o", "messages": [{"role": "user", "content": "Hello"}]}\'\n'})}),"\n",(0,r.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,r.jsxs)(n.p,{children:["See ",(0,r.jsx)(n.a,{href:"/docs/Guides/Deployment",children:"Deployment"})," for installation instructions and ",(0,r.jsx)(n.a,{href:"/docs/Guides/Configuration",children:"Configuration"})," for setup details."]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);