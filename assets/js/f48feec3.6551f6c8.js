"use strict";(globalThis.webpackChunkcoo_llm_docs=globalThis.webpackChunkcoo_llm_docs||[]).push([[2419],{891:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"User-Guide/Practical-Usage","title":"Practical Usage Guide","description":"Practical guide to implementing COO-LLM in real applications with tips and best practices","source":"@site/content/User-Guide/Practical-Usage.md","sourceDirName":"User-Guide","slug":"/User-Guide/Practical-Usage","permalink":"/docs/docs/User-Guide/Practical-Usage","draft":false,"unlisted":false,"editUrl":"https://github.com/coo-llm/coo-llm-main/tree/main/docs/content/content/User-Guide/Practical-Usage.md","tags":[{"inline":true,"label":"user-guide","permalink":"/docs/docs/tags/user-guide"},{"inline":true,"label":"practical","permalink":"/docs/docs/tags/practical"},{"inline":true,"label":"implementation","permalink":"/docs/docs/tags/implementation"},{"inline":true,"label":"tips","permalink":"/docs/docs/tags/tips"}],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"tags":["user-guide","practical","implementation","tips"],"description":"Practical guide to implementing COO-LLM in real applications with tips and best practices","keywords":["practical usage","implementation","tips","tricks","patterns"]},"sidebar":"tutorialSidebar","previous":{"title":"API-Usage","permalink":"/docs/docs/User-Guide/API-Usage"},"next":{"title":"Providers","permalink":"/docs/docs/Guides/Providers"}}');var s=t(4848),r=t(8453);const a={sidebar_position:2,tags:["user-guide","practical","implementation","tips"],description:"Practical guide to implementing COO-LLM in real applications with tips and best practices",keywords:["practical usage","implementation","tips","tricks","patterns"]},l="Practical Usage Guide",o={},c=[{value:"\ud83c\udfaf Choosing the Right Model",id:"-choosing-the-right-model",level:2},{value:"Use Case Decision Tree",id:"use-case-decision-tree",level:3},{value:"Cost Optimization Strategies",id:"cost-optimization-strategies",level:3},{value:"\ud83d\ude80 Implementation Patterns",id:"-implementation-patterns",level:2},{value:"Streaming for Better UX",id:"streaming-for-better-ux",level:3},{value:"Error Handling &amp; Resilience",id:"error-handling--resilience",level:3},{value:"\ud83c\udfa8 Advanced Prompting Techniques",id:"-advanced-prompting-techniques",level:2},{value:"Chain of Thought Prompting",id:"chain-of-thought-prompting",level:3},{value:"Few-Shot Learning",id:"few-shot-learning",level:3},{value:"\u26a1 Performance Optimization",id:"-performance-optimization",level:2},{value:"Parameter Tuning Guide",id:"parameter-tuning-guide",level:3},{value:"Batch Processing",id:"batch-processing",level:3},{value:"Caching Strategies",id:"caching-strategies",level:3},{value:"\ud83d\udd12 Security Best Practices",id:"-security-best-practices",level:2},{value:"API Key Management",id:"api-key-management",level:3},{value:"Input Validation &amp; Sanitization",id:"input-validation--sanitization",level:3},{value:"\ud83d\udcca Monitoring &amp; Observability",id:"-monitoring--observability",level:2},{value:"Usage Tracking",id:"usage-tracking",level:3},{value:"Health Checks",id:"health-checks",level:3},{value:"\ud83d\udee0\ufe0f How COO-LLM Works Internally",id:"\ufe0f-how-coo-llm-works-internally",level:2},{value:"Load Balancing Algorithm",id:"load-balancing-algorithm",level:3},{value:"Rate Limiting Implementation",id:"rate-limiting-implementation",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"practical-usage-guide",children:"Practical Usage Guide"})}),"\n",(0,s.jsx)(n.p,{children:"This guide focuses on real-world implementation patterns, tips & tricks, and how to get the most out of COO-LLM in your applications."}),"\n",(0,s.jsx)(n.h2,{id:"-choosing-the-right-model",children:"\ud83c\udfaf Choosing the Right Model"}),"\n",(0,s.jsx)(n.h3,{id:"use-case-decision-tree",children:"Use Case Decision Tree"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Need high-quality reasoning?\n\u251c\u2500\u2500 Yes \u2192 Use GPT-4o or Claude 3 Opus\n\u2502   \u2514\u2500\u2500 Budget constraints? \u2192 GPT-4o (cheaper)\n\u2502       \u2514\u2500\u2500 Need multimodal? \u2192 GPT-4o Vision\n\u2514\u2500\u2500 No, need speed/cost optimization?\n    \u251c\u2500\u2500 Creative writing/code \u2192 Gemini 1.5 Pro\n    \u251c\u2500\u2500 Simple Q&A \u2192 Gemini 1.0 Pro or GPT-3.5\n    \u2514\u2500\u2500 Factual tasks \u2192 Any model with low temp\n"})}),"\n",(0,s.jsx)(n.h3,{id:"cost-optimization-strategies",children:"Cost Optimization Strategies"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"1. Model Fallback Chain"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Try expensive model first, fallback to cheaper ones\nmodels_to_try = [\n    "openai:gpt-4o",           # $0.03/1k tokens\n    "claude:claude-3-sonnet",  # $0.015/1k tokens\n    "gemini:gemini-1.5-pro"    # $0.007/1k tokens\n]\n\nfor model in models_to_try:\n    try:\n        response = call_coo_llm(model, prompt)\n        return response\n    except Exception as e:\n        logger.warning(f"{model} failed: {e}")\n        continue\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"2. Dynamic Model Selection"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def select_model_by_complexity(text):\n    word_count = len(text.split())\n\n    if word_count < 100:\n        return "gemini:gemini-1.0-pro"  # Fast & cheap\n    elif word_count < 500:\n        return "claude:claude-3-haiku"  # Good balance\n    else:\n        return "openai:gpt-4o"         # Complex reasoning needed\n'})}),"\n",(0,s.jsx)(n.h2,{id:"-implementation-patterns",children:"\ud83d\ude80 Implementation Patterns"}),"\n",(0,s.jsx)(n.h3,{id:"streaming-for-better-ux",children:"Streaming for Better UX"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why streaming matters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"User Experience"}),": See responses in real-time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Apparent Speed"}),": Feels faster even if total time is same"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory Efficiency"}),": Process large responses without loading everything"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cancellation"}),": Users can stop generation early"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Implementation:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"async function streamChat(model, messages) {\n  const response = await fetch('/v1/chat/completions', {\n    method: 'POST',\n    headers: {\n      'Authorization': `Bearer ${API_KEY}`,\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: model,\n      messages: messages,\n      stream: true,\n      max_tokens: 1000\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  let fullContent = '';\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n');\n\n    for (const line of lines) {\n      if (line.startsWith('data: ')) {\n        const data = line.slice(6);\n        if (data === '[DONE]') return fullContent;\n\n        try {\n          const parsed = JSON.parse(data);\n          const delta = parsed.choices[0]?.delta?.content || '';\n          fullContent += delta;\n\n          // Update UI in real-time\n          updateUI(fullContent);\n        } catch (e) {\n          // Handle parse errors\n        }\n      }\n    }\n  }\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"error-handling--resilience",children:"Error Handling & Resilience"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Circuit Breaker Pattern:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class COOLLMClient:\n    def __init__(self):\n        self.failure_count = 0\n        self.last_failure_time = 0\n        self.circuit_open = False\n\n    def call_with_circuit_breaker(self, model, messages):\n        if self.circuit_open:\n            if time.time() - self.last_failure_time > 60:  # Reset after 1min\n                self.circuit_open = False\n                self.failure_count = 0\n            else:\n                raise Exception("Circuit breaker open")\n\n        try:\n            response = self._call_api(model, messages)\n            self.failure_count = 0  # Reset on success\n            return response\n        except Exception as e:\n            self.failure_count += 1\n            self.last_failure_time = time.time()\n\n            if self.failure_count >= 5:  # Open circuit after 5 failures\n                self.circuit_open = True\n\n            raise e\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Exponential Backoff with Jitter:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import random\nimport time\n\ndef call_with_backoff(model, messages, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            return call_coo_llm(model, messages)\n        except RateLimitError:\n            if attempt == max_retries - 1:\n                raise\n\n            # Exponential backoff with jitter\n            base_delay = 2 ** attempt\n            jitter = random.uniform(0, 1)\n            delay = base_delay + jitter\n\n            print(f"Rate limited, waiting {delay:.2f}s...")\n            time.sleep(delay)\n        except Exception as e:\n            # Don\'t retry on auth/other errors\n            if "unauthorized" in str(e).lower():\n                raise\n            if attempt < max_retries - 1:\n                time.sleep(1)\n                continue\n            raise\n'})}),"\n",(0,s.jsx)(n.h2,{id:"-advanced-prompting-techniques",children:"\ud83c\udfa8 Advanced Prompting Techniques"}),"\n",(0,s.jsx)(n.h3,{id:"chain-of-thought-prompting",children:"Chain of Thought Prompting"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def analyze_code_with_cot(code_snippet):\n    prompt = f"""\nAnalyze this code step by step:\n\n{code_snippet}\n\nThink through your analysis:\n1. What does this code do?\n2. Are there any bugs or issues?\n3. How could it be improved?\n4. What are the performance implications?\n\nProvide your final assessment after thinking through each point.\n"""\n\n    return call_coo_llm("openai:gpt-4o", [{"role": "user", "content": prompt}])\n'})}),"\n",(0,s.jsx)(n.h3,{id:"few-shot-learning",children:"Few-Shot Learning"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def classify_sentiment(text):\n    examples = """\nPositive: "I love this product! It\'s amazing!" \u2192 positive\nNegative: "This is terrible, complete waste of money" \u2192 negative\nNeutral: "The product works as expected" \u2192 neutral\n\nText: "{text}"\nClassification:"""\n\n    prompt = examples.format(text=text)\n    response = call_coo_llm("gemini:gemini-1.5-pro",\n                          [{"role": "user", "content": prompt}])\n\n    return response.choices[0].message.content.strip().lower()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"-performance-optimization",children:"\u26a1 Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"parameter-tuning-guide",children:"Parameter Tuning Guide"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Low Values"}),(0,s.jsx)(n.th,{children:"High Values"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"temperature"})}),(0,s.jsx)(n.td,{children:"0.0-0.3"}),(0,s.jsx)(n.td,{children:"0.7-1.0"}),(0,s.jsx)(n.td,{children:"Factual \u2192 Creative"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"top_p"})}),(0,s.jsx)(n.td,{children:"0.1-0.5"}),(0,s.jsx)(n.td,{children:"0.9-1.0"}),(0,s.jsx)(n.td,{children:"Focused \u2192 Diverse"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"max_tokens"})}),(0,s.jsx)(n.td,{children:"50-200"}),(0,s.jsx)(n.td,{children:"1000-4000"}),(0,s.jsx)(n.td,{children:"Concise \u2192 Detailed"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"frequency_penalty"})}),(0,s.jsx)(n.td,{children:"0.0-0.5"}),(0,s.jsx)(n.td,{children:"1.0-2.0"}),(0,s.jsx)(n.td,{children:"Repetitive \u2192 Varied"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"batch-processing",children:"Batch Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'async def process_batch(prompts, batch_size=5):\n    """Process multiple prompts efficiently"""\n    semaphore = asyncio.Semaphore(batch_size)  # Limit concurrent requests\n\n    async def process_single(prompt):\n        async with semaphore:\n            return await call_coo_llm_async("gemini:gemini-1.0-pro", prompt)\n\n    tasks = [process_single(prompt) for prompt in prompts]\n    return await asyncio.gather(*tasks)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"caching-strategies",children:"Caching Strategies"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import hashlib\n\nclass ResponseCache:\n    def __init__(self):\n        self.cache = {}\n\n    def get_cache_key(self, model, messages, params):\n        # Create deterministic key from inputs\n        key_data = f"{model}:{messages}:{params}"\n        return hashlib.md5(key_data.encode()).hexdigest()\n\n    def get_cached_response(self, model, messages, params):\n        key = self.get_cache_key(model, messages, params)\n        return self.cache.get(key)\n\n    def cache_response(self, model, messages, params, response):\n        key = self.get_cache_key(model, messages, params)\n        self.cache[key] = response\n'})}),"\n",(0,s.jsx)(n.h2,{id:"-security-best-practices",children:"\ud83d\udd12 Security Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"api-key-management",children:"API Key Management"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Rotate keys automatically\nclass KeyManager:\n    def __init__(self):\n        self.keys = {\n            'openai': ['sk-key1', 'sk-key2', 'sk-key3'],\n            'gemini': ['gemini-key1']\n        }\n        self.current_key_index = {}\n\n    def get_next_key(self, provider):\n        if provider not in self.keys:\n            raise ValueError(f\"No keys for provider {provider}\")\n\n        keys = self.keys[provider]\n        current = self.current_key_index.get(provider, 0)\n        key = keys[current]\n\n        # Round-robin to next key\n        self.current_key_index[provider] = (current + 1) % len(keys)\n\n        return key\n"})}),"\n",(0,s.jsx)(n.h3,{id:"input-validation--sanitization",children:"Input Validation & Sanitization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def sanitize_input(text, max_length=10000):\n    """Clean and validate input text"""\n    if not text or not isinstance(text, str):\n        raise ValueError("Invalid input: must be non-empty string")\n\n    # Remove potentially harmful content\n    text = text.strip()\n    text = re.sub(r\'[\\\\x00-\\\\x1f\\\\x7f-\\\\x9f]\', \'\', text)  # Remove control chars\n\n    if len(text) > max_length:\n        raise ValueError(f"Input too long: {len(text)} > {max_length}")\n\n    return text\n'})}),"\n",(0,s.jsx)(n.h2,{id:"-monitoring--observability",children:"\ud83d\udcca Monitoring & Observability"}),"\n",(0,s.jsx)(n.h3,{id:"usage-tracking",children:"Usage Tracking"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class UsageTracker:\n    def __init__(self):\n        self.usage = defaultdict(lambda: {'tokens': 0, 'cost': 0.0, 'calls': 0})\n\n    def track_usage(self, model, response):\n        provider = model.split(':')[0]\n        usage = response.get('usage', {})\n\n        self.usage[provider]['tokens'] += usage.get('total_tokens', 0)\n        self.usage[provider]['calls'] += 1\n\n        # Estimate cost (simplified)\n        cost_per_token = 0.002  # Example rate\n        self.usage[provider]['cost'] += usage.get('total_tokens', 0) * cost_per_token\n\n    def get_report(self):\n        return dict(self.usage)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"health-checks",children:"Health Checks"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'async def health_check():\n    """Comprehensive health check"""\n    checks = {\n        \'api_reachable\': False,\n        \'providers_healthy\': {},\n        \'response_time\': None\n    }\n\n    start_time = time.time()\n\n    try:\n        # Test basic API call\n        response = await call_coo_llm("gemini:gemini-1.0-pro",\n                                    [{"role": "user", "content": "test"}])\n        checks[\'api_reachable\'] = True\n        checks[\'response_time\'] = time.time() - start_time\n\n        # Test each provider\n        providers = [\'openai\', \'gemini\', \'claude\']\n        for provider in providers:\n            try:\n                await call_coo_llm(f"{provider}:test-model",\n                                 [{"role": "user", "content": "test"}])\n                checks[\'providers_healthy\'][provider] = True\n            except:\n                checks[\'providers_healthy\'][provider] = False\n\n    except Exception as e:\n        checks[\'error\'] = str(e)\n\n    return checks\n'})}),"\n",(0,s.jsx)(n.h2,{id:"\ufe0f-how-coo-llm-works-internally",children:"\ud83d\udee0\ufe0f How COO-LLM Works Internally"}),"\n",(0,s.jsx)(n.h3,{id:"load-balancing-algorithm",children:"Load Balancing Algorithm"}),"\n",(0,s.jsxs)(n.p,{children:["COO-LLM uses a ",(0,s.jsx)(n.strong,{children:"hybrid balancing strategy"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cost-First"}),": Routes to cheapest available provider"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance-Aware"}),": Considers response times and success rates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rate Limit Conscious"}),": Avoids keys nearing limits"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Failover Ready"}),": Automatically switches on failures"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Decision Flow:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"New request arrives\n\u251c\u2500\u2500 Check cache (semantic similarity)\n\u251c\u2500\u2500 Select provider based on:\n\u2502   \u251c\u2500\u2500 Model availability\n\u2502   \u251c\u2500\u2500 Cost optimization\n\u2502   \u251c\u2500\u2500 Current load & rate limits\n\u2502   \u2514\u2500\u2500 Recent performance metrics\n\u251c\u2500\u2500 Choose API key (round-robin/load-based)\n\u251c\u2500\u2500 Make request with retry logic\n\u251c\u2500\u2500 Cache successful response\n\u2514\u2500\u2500 Update metrics\n"})}),"\n",(0,s.jsx)(n.h3,{id:"rate-limiting-implementation",children:"Rate Limiting Implementation"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Multi-Level Rate Limiting:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Provider Level"}),": Respects external API limits"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Key Level"}),": Distributes load across your keys"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Client Level"}),": Per-user/application limits"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Global Level"}),": Overall system protection"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Token Bucket Algorithm:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Simplified token bucket implementation\nclass TokenBucket:\n    def __init__(self, capacity, refill_rate):\n        self.capacity = capacity\n        self.tokens = capacity\n        self.refill_rate = refill_rate  # tokens per second\n        self.last_refill = time.time()\n\n    def consume(self, tokens_needed):\n        now = time.time()\n        elapsed = now - self.last_refill\n\n        # Refill tokens\n        self.tokens = min(self.capacity,\n                         self.tokens + elapsed * self.refill_rate)\n        self.last_refill = now\n\n        if self.tokens >= tokens_needed:\n            self.tokens -= tokens_needed\n            return True\n        return False\n"})}),"\n",(0,s.jsx)(n.p,{children:"This internal knowledge helps you understand why certain behaviors occur and how to optimize your usage patterns."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>l});var i=t(6540);const s={},r=i.createContext(s);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);